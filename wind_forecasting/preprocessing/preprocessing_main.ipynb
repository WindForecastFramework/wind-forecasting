{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! module load mambaforge or mamba\n",
    "# ! mamba create -n wind_forecasting_env python=3.12\n",
    "# ! mamba activate wind_forecasting_env\n",
    "# ! conda install -c conda-forge jupyterlab mpi4py impi_rt\n",
    "# git clone https://github.com/achenry/wind-forecasting.git\n",
    "# git checkout feature/nacelle_calibration\n",
    "# git submodule update --init --recursive\n",
    "# ! pip install ./OpenOA # have to change pyproject.toml to allow for python 3.12.7\n",
    "# ! pip install floris polars windrose netCDF4 statsmodels h5pyd seaborn pyarrow memory_profiler scikit-learn\n",
    "# ! python -m ipykernel install --user --name=wind_forecasting_env\n",
    "# ./run_jupyter_preprocessing.sh && http://localhost:7878/lab\n",
    "\n",
    "#%load_ext memory_profiler\n",
    "from data_loader import DataLoader\n",
    "from data_filter import DataFilter\n",
    "from data_inspector import DataInspector\n",
    "from openoa.utils import plot, filters, power_curve\n",
    "import polars.selectors as cs\n",
    "import polars as pl\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sys import platform\n",
    "import os\n",
    "import logging\n",
    "from datetime import timedelta\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# import datetime\n",
    "# t2 = datetime.datetime(2022, 3, 1, 12, 0, 55)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Print NetCDF Data Structure, Load Data, Transform Datetime Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PLOT = False\n",
    "RELOAD_DATA = False\n",
    "\n",
    "if platform == \"darwin\":\n",
    "    DATA_DIR = \"/Users/ahenry/Documents/toolboxes/wind_forecasting/examples/data\"\n",
    "    # PL_SAVE_PATH = \"/Users/ahenry/Documents/toolboxes/wind_forecasting/examples/data/kp.turbine.zo2.b0.raw.parquet\"\n",
    "    # FILE_SIGNATURE = \"kp.turbine.z02.b0.*.*.*.nc\"\n",
    "    PL_SAVE_PATH = \"/Users/ahenry/Documents/toolboxes/wind_forecasting/examples/data/kp.turbine.zo2.b0.raw.parquet\"\n",
    "    FILE_SIGNATURE = \"kp.turbine.z02.b0.202203*.*.*.nc\"\n",
    "    MULTIPROCESSOR = \"cf\"\n",
    "    TURBINE_INPUT_FILEPATH = \"/Users/ahenry/Documents/toolboxes/wind_forecasting/examples/inputs/ge_282_127.yaml\"\n",
    "    FARM_INPUT_FILEPATH = \"/Users/ahenry/Documents/toolboxes/wind_forecasting/examples/inputs/gch_KP_v4.yaml\"\n",
    "    FEATURES = [\"time\", \"turbine_id\", \"turbine_status\", \"wind_direction\", \"wind_speed\", \"power_output\", \"nacelle_direction\"]\n",
    "    WIDE_FORMAT = False\n",
    "    COLUMN_MAPPING = {\"time\": \"date\",\n",
    "                            \"turbine_id\": \"turbine_id\",\n",
    "                            \"turbine_status\": \"WTUR.TurSt\",\n",
    "                            \"wind_direction\": \"WMET.HorWdDir\",\n",
    "                            \"wind_speed\": \"WMET.HorWdSpd\",\n",
    "                            \"power_output\": \"WTUR.W\",\n",
    "                            \"nacelle_direction\": \"WNAC.Dir\"\n",
    "                            }\n",
    "elif platform == \"linux\":\n",
    "    DATA_DIR = \"/pl/active/paolab/awaken_data/kp.turbine.z02.b0/\"\n",
    "    PL_SAVE_PATH = \"/scratch/alpine/aohe7145/awaken_data/kp.turbine.zo2.b0.raw.parquet\"\n",
    "    FILE_SIGNATURE = \"kp.turbine.z02.b0.*.*.*.nc\"\n",
    "    MULTIPROCESSOR = \"mpi\"\n",
    "    TURBINE_INPUT_FILEPATH = \"/projects/aohe7145/toolboxes/wind-forecasting/examples/inputs/ge_282_127.yaml\"\n",
    "    FARM_INPUT_FILEPATH = \"/projects/aohe7145/toolboxes/wind-forecasting/examples/inputs/gch_KP_v4.yaml\"\n",
    "    FEATURES = [\"time\", \"turbine_id\", \"turbine_status\", \"wind_direction\", \"wind_speed\", \"power_output\", \"nacelle_direction\"]\n",
    "    WIDE_FORMAT = False\n",
    "    COLUMN_MAPPING = {\"time\": \"date\",\n",
    "                            \"turbine_id\": \"turbine_id\",\n",
    "                            \"turbine_status\": \"WTUR.TurSt\",\n",
    "                            \"wind_direction\": \"WMET.HorWdDir\",\n",
    "                            \"wind_speed\": \"WMET.HorWdSpd\",\n",
    "                            \"power_output\": \"WTUR.W\",\n",
    "                            \"nacelle_direction\": \"WNAC.Dir\"\n",
    "                            }\n",
    "\n",
    "DT = 5\n",
    "if FILE_SIGNATURE.endswith(\".nc\"):\n",
    "    DATA_FORMAT = \"netcdf\"\n",
    "elif FILE_SIGNATURE.endswith(\".csv\"):\n",
    "    DATA_FORMAT = \"csv\"\n",
    "else:\n",
    "    raise ValueError(\"Invalid file signature. Please specify either '*.nc' or '*.csv'.\")\n",
    "# turbine_ids =  [\"wt028\", \"wt033\", \"wt073\"]\n",
    "                         \n",
    "data_loader = DataLoader(\n",
    "                data_dir=DATA_DIR,\n",
    "                file_signature=FILE_SIGNATURE,\n",
    "                save_path=PL_SAVE_PATH,\n",
    "                multiprocessor=MULTIPROCESSOR,\n",
    "                dt=DT,\n",
    "                desired_feature_types=FEATURES,\n",
    "                data_format=DATA_FORMAT,\n",
    "                column_mapping=COLUMN_MAPPING,\n",
    "                wide_format=WIDE_FORMAT,\n",
    "                ffill_limit=int(60 * 60 * 10 // DT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader.print_netcdf_structure(data_loader.file_paths[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not RELOAD_DATA and os.path.exists(data_loader.save_path):\n",
    "     # Note that the order of the columns in the provided schema must match the order of the columns in the CSV being read.\n",
    "    schema = pl.Schema(dict(sorted(({**{\"time\": pl.Datetime(time_unit=\"ms\")},\n",
    "                **{\n",
    "                    f\"{feat}_{tid}\": pl.Float64\n",
    "                    for feat in FEATURES \n",
    "                    for tid in [f\"wt{d+1:03d}\" for d in range(88)]}\n",
    "                }).items())))\n",
    "    logging.info(\"üîÑ Loading existing Parquet file\")\n",
    "    df_query = pl.scan_parquet(source=data_loader.save_path)\n",
    "    logging.info(\"‚úÖ Loaded existing Parquet file successfully\")\n",
    "    data_loader.available_features = sorted(df_query.collect_schema().names())\n",
    "    data_loader.turbine_ids = sorted(set(col.split(\"_\")[-1] for col in data_loader.available_features if \"wt\" in col))\n",
    "else:\n",
    "    logging.info(\"üîÑ Processing new data files\")\n",
    "    df_query = data_loader.read_multi_files()\n",
    "        \n",
    "    if df_query is not None:\n",
    "        # Perform any additional operations on df_query if needed\n",
    "        logging.info(\"‚úÖ Data processing completed successfully\")\n",
    "    else:\n",
    "        logging.warning(\"‚ö†Ô∏è No data was processed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Wind Farm, Data Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_inspector = DataInspector(turbine_input_filepath=TURBINE_INPUT_FILEPATH, farm_input_filepath=FARM_INPUT_FILEPATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if PLOT:\n",
    "    data_inspector.plot_wind_farm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if PLOT:\n",
    "    data_inspector.plot_wind_speed_power(df_query, turbine_ids=[\"wt073\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if PLOT:\n",
    "    data_inspector.plot_wind_speed_weibull(df_query, turbine_ids=\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if PLOT:\n",
    "    data_inspector.plot_wind_rose(df_query, turbine_ids=\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if PLOT:\n",
    "    data_inspector.plot_correlation(df_query, \n",
    "    DataInspector.get_features(df_query, feature_types=[\"wind_speed\", \"wind_direction\", \"nacelle_direction\"], turbine_ids=[\"wt073\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " if PLOT:\n",
    "    data_inspector.plot_boxplot_wind_speed_direction(df_query, turbine_ids=[\"wt073\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if PLOT:\n",
    "    data_inspector.plot_time_series(df_query, turbine_ids=[\"wt073\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenOA Data Preparation & Inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ws_cols = DataInspector.get_features(df_query, \"wind_speed\")\n",
    "wd_cols = DataInspector.get_features(df_query, \"wind_direction\")\n",
    "pwr_cols = DataInspector.get_features(df_query, \"power_output\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Features of interest = {data_loader.desired_feature_types}\")\n",
    "print(f\"Available features = {data_loader.available_features}\")\n",
    "# qa.describe(DataInspector.collect_data(df=df_query))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if PLOT:\n",
    "    plot.column_histograms(DataInspector.collect_data(df=df_query, \n",
    "    feature_types=DataInspector.get_features(df_query, [\"wind_speed\", \"wind_direction\", \"power_output\", \"nacelle_direction\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_filter = DataFilter(turbine_availability_col=None, turbine_status_col=\"turbine_status\", multiprocessor=MULTIPROCESSOR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_query.filter(pl.col(\"time\") <= t2).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nullify Inoperational Turbine Rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if wind speed/dir measurements from inoperational turbines differ from fully operational \n",
    "# df_query = data_filter.filter_inoperational(df_query, status_codes=[1], include_nan=False)\n",
    "status_codes = [1]\n",
    "mask = lambda tid: pl.col(f\"turbine_status_{tid}\").is_in(status_codes) | pl.col(f\"turbine_status_{tid}\").is_null()\n",
    "features = ws_cols\n",
    "\n",
    "if PLOT:\n",
    "    DataInspector.print_pc_unfiltered_vals(df_query, features, mask)\n",
    "    data_inspector.plot_filtered_vs_unfiltered(df_query, mask, ws_cols + wd_cols, [\"wind_speed\", \"wind_direction\"], [\"Wind Speed [m/s]\", \"Wind Direction [deg]\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop through each turbine's wind speed and wind direction columns, and compare the distribution of data with and without the inoperational turbines\n",
    "# fill out_of_range measurements with Null st they are marked for interpolation via impute or linear/forward fill interpolation later\n",
    "threshold = 0.01\n",
    "df_query = data_filter.conditional_filter(df_query, threshold, mask, ws_cols + wd_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_query.head().collect()\n",
    "# df_query.filter(pl.col(\"time\") <= t2).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wind Speed Range Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# check for wind speed values that are outside of the acceptable range\n",
    "if RELOAD_DATA or not os.path.exists(os.path.join(DATA_DIR, \"out_of_range.npy\")):\n",
    "    ws = DataInspector.collect_data(df=df_query, feature_types=\"wind_speed\")\n",
    "    out_of_range = (filters.range_flag(ws, lower=0, upper=70) & ~ws.isna()).values # range flag includes formerly null values as nan\n",
    "    del ws\n",
    "    np.save(os.path.join(DATA_DIR, \"out_of_range.npy\"), out_of_range)\n",
    "else:\n",
    "  out_of_range = np.load(os.path.join(DATA_DIR, \"out_of_range.npy\"))\n",
    "# qa.describe(DataInspector.collect_data(df=df_query, feature_types=\"wind_speed\", mask=np.any(out_of_range, axis=1)))\n",
    "\n",
    "# check if wind speed/dir measurements from inoperational turbines differ from fully operational \n",
    "mask = lambda tid: ~out_of_range[:, data_loader.turbine_ids.index(tid)]\n",
    "features = ws_cols\n",
    "\n",
    "if PLOT:\n",
    "    DataInspector.print_pc_unfiltered_vals(df_query, features, mask)\n",
    "    data_inspector.plot_filtered_vs_unfiltered(df_query, mask, ws_cols, [\"wind_speed\"], [\"Wind Speed [m/s]\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop through each turbine's wind speed and wind direction columns, and compare the distribution of data with and without the inoperational turbines\n",
    "# fill out_of_range measurements with Null st they are marked for interpolation via impute or linear/forward fill interpolation later\n",
    "threshold = 0.01\n",
    "df_query = data_filter.conditional_filter(df_query, threshold, mask, ws_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del out_of_range \n",
    "df_query.head().collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Power Curve Window Range Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply a window range filter to remove data with power values outside of the window from 20 to 3000 kW for wind speeds between 5 and 40 m/s.\n",
    "# identifies when turbine is shut down, filtering for normal turbine operation\n",
    "if RELOAD_DATA or not os.path.exists(os.path.join(DATA_DIR, \"out_of_window.npy\")):\n",
    "  out_of_window = np.stack([(filters.window_range_flag(window_col=DataInspector.collect_data(df=df_query, \n",
    "                                                                                    feature_types=[\"wind_speed\"], \n",
    "                                                                                    turbine_ids=[tid])[f\"wind_speed_{tid}\"],\n",
    "                                                    window_start=5., window_end=40., \n",
    "                                                    value_col=DataInspector.collect_data(df=df_query, \n",
    "                                                                                    feature_types=[\"power_output\"], \n",
    "                                                                                    turbine_ids=[tid])[f\"power_output_{tid}\"],\n",
    "                                                    value_min=20., value_max=3000.)\n",
    "                        & df_query.select(no_nulls=pl.all_horizontal(pl.col(f\"wind_speed_{tid}\").is_not_null(), pl.col(f\"power_output_{tid}\").is_not_null()))\\\n",
    "                                  .collect(streaming=True).to_pandas()[\"no_nulls\"]\n",
    "                                #   & ~DataInspector.collect_data(df=df_query, feature_types=\"wind_speed\", turbine_ids=tid).isna()\n",
    "                                #   & ~DataInspector.collect_data(df=df_query, feature_types=\"power_output\", turbine_ids=tid).isna()\n",
    "                                  ).values for tid in data_loader.turbine_ids], axis=1)\n",
    "\n",
    "  np.save(os.path.join(DATA_DIR, \"out_of_window.npy\"), out_of_window)\n",
    "else:\n",
    "  out_of_window = np.load(os.path.join(DATA_DIR, \"out_of_window.npy\"))\n",
    "\n",
    "# check if wind speed/dir measurements from inoperational turbines differ from fully operational \n",
    "mask = lambda tid: ~out_of_window[:, data_loader.turbine_ids.index(tid)]\n",
    "features = ws_cols \n",
    "\n",
    "if PLOT:\n",
    "  DataInspector.print_pc_unfiltered_vals(df_query, features, mask)\n",
    "  data_inspector.plot_filtered_vs_unfiltered(df_query, mask, features, features, [\"Wind Speed [m/s]\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if PLOT:\n",
    "    # plot values that are outside of power-wind speed range\n",
    "    plot.plot_power_curve(\n",
    "        DataInspector.collect_data(df=df_query, feature_types=\"wind_speed\").to_numpy().flatten(),\n",
    "        DataInspector.collect_data(df=df_query, feature_types=\"power_output\").to_numpy().flatten(),\n",
    "        flag=out_of_window.flatten(),\n",
    "        flag_labels=(\"Outside Acceptable Window\", \"Acceptable Power Curve Points\"),\n",
    "        xlim=(-1, 15),\n",
    "        ylim=(-100, 3000),\n",
    "        legend=True,\n",
    "        scatter_kwargs=dict(alpha=0.4, s=10)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill cells corresponding to values that are outside of power-wind speed window range with Null st they are marked for interpolation via impute or linear/forward fill interpolation later\n",
    "# loop through each turbine's wind speed and wind direction columns, and compare the distribution of data with and without the inoperational turbines\n",
    "threshold = 0.01\n",
    "df_query = data_filter.conditional_filter(df_query, threshold, mask, features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del out_of_window\n",
    "df_query.head().collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Power Curve Bin Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply a bin filter to remove data with power values outside of an envelope around median power curve at each wind speed\n",
    "if RELOAD_DATA or not os.path.exists(os.path.join(DATA_DIR, \"bin_outliers.npy\")):\n",
    "  bin_outliers = np.stack([(filters.bin_filter(\n",
    "                                    bin_col=f\"power_output_{tid}\", \n",
    "                                    value_col=f\"wind_speed_{tid}\", \n",
    "                                    bin_width=50, threshold=3,\n",
    "                                    center_type=\"median\", \n",
    "                                    bin_min=20., bin_max=0.90*(df_query.select(f\"power_output_{tid}\").max().collect().item() or 3000.),\n",
    "                                    threshold_type=\"scalar\", direction=\"below\",\n",
    "                                    data=DataInspector.collect_data(df=df_query, \n",
    "                                                                    feature_types=[\"wind_speed\", \"power_output\"], \n",
    "                                                                    turbine_ids=[tid])\n",
    "                                    )\n",
    "                                  & df_query.select(no_nulls=pl.all_horizontal(pl.col(f\"wind_speed_{tid}\").is_not_null(), pl.col(f\"power_output_{tid}\").is_not_null()))\\\n",
    "                                    .collect().to_pandas()[\"no_nulls\"]\n",
    "                                    ).values for tid in data_loader.turbine_ids], axis=1)\n",
    "  np.save(os.path.join(DATA_DIR, \"bin_outliers.npy\"), bin_outliers)\n",
    "else:\n",
    "  bin_outliers = np.load(os.path.join(DATA_DIR, \"bin_outliers.npy\"))\n",
    "\n",
    "# check if wind speed/dir measurements from inoperational turbines differ from fully operational \n",
    "mask = lambda tid: ~bin_outliers[:, data_loader.turbine_ids.index(tid)]\n",
    "# TODO save filters to npy files and only reload if necessary\n",
    "features = ws_cols\n",
    "\n",
    "if PLOT:\n",
    "  DataInspector.print_pc_unfiltered_vals(df_query, features, mask)\n",
    "  data_inspector.plot_filtered_vs_unfiltered(df_query, mask, features, features, [\"Wind Speed [m/s]\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if PLOT:\n",
    "    # plot values outside the power-wind speed bin filter\n",
    "    plot.plot_power_curve(\n",
    "        DataInspector.collect_data(df=df_query, feature_types=\"wind_speed\").to_numpy().flatten(),\n",
    "        DataInspector.collect_data(df=df_query, feature_types=\"power_output\").to_numpy().flatten(),\n",
    "        flag=bin_outliers.flatten(),\n",
    "        flag_labels=(\"Anomylous Data\", \"Normal Wind Speed Sensor Operation\"),\n",
    "        xlim=(-1, 15),\n",
    "        ylim=(-100, 3000),\n",
    "        legend=True,\n",
    "        scatter_kwargs=dict(alpha=0.4, s=10)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill cells corresponding to values that are outside of power-wind speed bins with Null st they are marked for interpolation via impute or linear/forward fill interpolation later\n",
    "# loop through each turbine's wind speed and wind direction columns, and compare the distribution of data with and without the inoperational turbines\n",
    "threshold = 0.01\n",
    "df_query = data_filter.conditional_filter(df_query, threshold, mask, features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del bin_outliers\n",
    "df_query.head().collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Power Curve Fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if PLOT:\n",
    "    # Fit the power curves\n",
    "    iec_curve = power_curve.IEC(\n",
    "        windspeed_col=\"wind_speed\", power_col=\"power_output\",\n",
    "        data=DataInspector.unpivot_dataframe(df_query).select(\"wind_speed\", \"power_output\").filter(pl.all_horizontal(pl.all().is_not_null())).collect(streaming=True).to_pandas(),\n",
    "        )\n",
    "\n",
    "    l5p_curve = power_curve.logistic_5_parametric(\n",
    "        windspeed_col=\"wind_speed\", power_col=\"power_output\",\n",
    "        data=DataInspector.unpivot_dataframe(df_query).select(\"wind_speed\", \"power_output\").filter(pl.all_horizontal(pl.all().is_not_null())).collect(streaming=True).to_pandas(),\n",
    "        )\n",
    "\n",
    "    spline_curve = power_curve.gam(\n",
    "        windspeed_col=\"wind_speed\", power_col=\"power_output\",\n",
    "        data=DataInspector.unpivot_dataframe(df_query).select(\"wind_speed\", \"power_output\").filter(pl.all_horizontal(pl.all().is_not_null())).collect(streaming=True).to_pandas(), \n",
    "        n_splines=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if PLOT:\n",
    "    fig, ax = plot.plot_power_curve(\n",
    "        DataInspector.collect_data(df=df_query, feature_types=\"wind_speed\").to_numpy().flatten(),\n",
    "        DataInspector.collect_data(df=df_query, feature_types=\"power_output\").to_numpy().flatten(),\n",
    "        flag=np.zeros(DataInspector.collect_data(df=df_query, feature_types=\"wind_speed\").shape[0], dtype=bool),\n",
    "        flag_labels=(\"\", \"Filtered Power Curve\"),\n",
    "        xlim=(-1, 15),  # optional input for refining plots\n",
    "        ylim=(-100, 3000),  # optional input for refining plots\n",
    "        legend=False,  # optional flag for adding a legend\n",
    "        scatter_kwargs=dict(alpha=0.4, s=10),  # optional input for refining plots\n",
    "        return_fig=True,\n",
    "    )\n",
    "\n",
    "    x = np.linspace(0, 20, 100)\n",
    "    ax.plot(x, iec_curve(x), color=\"red\", label = \"IEC\", linewidth = 3)\n",
    "    ax.plot(x, spline_curve(x), color=\"C1\", label = \"Spline\", linewidth = 3)\n",
    "    ax.plot(x, l5p_curve(x), color=\"C2\", label = \"L5P\", linewidth = 3)\n",
    "\n",
    "    ax.legend()\n",
    "\n",
    "    fig.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unresponsive Sensor Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find stuck sensor measurements for each turbine and set them to null\n",
    "# TODO question does unresponsive flag include nan values\n",
    "# frozen_thresholds = [(data_loader.ffill_limit * i) + 1 for i in range(1, 19)]\n",
    "# print(frozen_thresholds)\n",
    "# ws_pcs = []\n",
    "# wd_pcs = []\n",
    "# pwr_pcs = []\n",
    "# for thr in frozen_thresholds:\n",
    "#     ws_frozen_sensor = filters.unresponsive_flag(data=DataInspector.collect_data(df=df_query, feature_types=\"wind_speed\"), threshold=thr).values\n",
    "#     wd_frozen_sensor = filters.unresponsive_flag(data=DataInspector.collect_data(df=df_query, feature_types=\"wind_direction\"), threshold=thr).values\n",
    "#     pwr_frozen_sensor = filters.unresponsive_flag(data=DataInspector.collect_data(df=df_query, feature_types=\"power_output\"), threshold=thr).values\n",
    "\n",
    "#     # check if wind speed/dir measurements from inoperational turbines differ from fully operational\n",
    "#     print(f\"For a threshold of {thr} for the frozen sensor filters:\")\n",
    "#     ws_pcs.append(DataInspector.print_pc_unfiltered_vals(df_query, ws_cols, lambda tid: ws_frozen_sensor[:, data_loader.turbine_ids.index(tid)]))\n",
    "#     wd_pcs.append(DataInspector.print_pc_unfiltered_vals(df_query, wd_cols, lambda tid: wd_frozen_sensor[:, data_loader.turbine_ids.index(tid)]))\n",
    "#     pwr_pcs.append(DataInspector.print_pc_unfiltered_vals(df_query, pwr_cols, lambda tid: pwr_frozen_sensor[:, data_loader.turbine_ids.index(tid)]))\n",
    "\n",
    "# fig, ax = plt.subplots(3, 1, sharex=True)\n",
    "# for t_idx in range(len(data_loader.turbine_ids)):\n",
    "#     ax[0].scatter(x=frozen_thresholds, y=[x[1][t_idx] for x in ws_pcs], label=data_loader.turbine_ids[t_idx])\n",
    "#     ax[1].scatter(x=frozen_thresholds, y=[x[1][t_idx] for x in wd_pcs], label=data_loader.turbine_ids[t_idx])\n",
    "#     ax[2].scatter(x=frozen_thresholds, y=[x[1][t_idx] for x in pwr_pcs],label=data_loader.turbine_ids[t_idx])\n",
    "\n",
    "# h, l = ax[0].get_legend_handles_labels()\n",
    "# ax[0].legend(h[:len(data_loader.turbine_ids)], l[:len(data_loader.turbine_ids)])\n",
    "# ax[0].set_title(\"Percentage of Unfrozen Wind Speed Measurements\")\n",
    "# ax[1].set_title(\"Percentage of Unfrozen Wind Direction Measurements\")\n",
    "# ax[2].set_title(\"Percentage of Unfrozen Power Output Measurements\")\n",
    "# qa.describe(pl.concat([DataInspector.collect_data(df=df_query, feature_types=feature_type, mask=mask, to_pandas=False)\n",
    "#                         for mask, feature_type in zip([ws_frozen_sensor, wd_frozen_sensor, pwr_frozen_sensor], [\"wind_speed\", \"wind_direction\", \"power_output\"])], \n",
    "#                             how=\"horizontal\")\\\n",
    "#                                .to_pandas())\n",
    "\n",
    "# TODO ASK ERIC if necessary\n",
    "if False:\n",
    "    thr = data_loader.ffill_limit + 1\n",
    "    # frozen_sensor = (filters.unresponsive_flag(data=df_query.select(features).collect(streaming=True).to_pandas(), threshold=thr)\n",
    "                                    # & df_query.select([pl.col(feat).is_not_null().alias(feat) for feat in features])\\\n",
    "                                    #   .collect(streaming=True).to_pandas()\n",
    "                                    #   ).values\n",
    "    features = ws_cols + wd_cols\n",
    "    # frozen_sensor = np.stack([(filters.unresponsive_flag(data=df_query.select(feat).collect(streaming=True).to_pandas(), threshold=thr)\n",
    "    #                                 & df_query.select(pl.col(feat).is_not_null().alias(feat))\\\n",
    "    #                                 .collect(streaming=True).to_pandas()\n",
    "    #                                 ).values for feat in features], axis=1)\n",
    "    # find values of wind speed/direction, where there are duplicate values with nulls inbetween\n",
    "    mask = lambda tid: ~frozen_sensor[:, data_loader.turbine_ids.index(tid)]\n",
    "\n",
    "# df_query.select([pl.col(feat).is_not_null().name.suffix(\"_not_null\") for feat in ws_cols])\\\n",
    "#                                   .collect(streaming=True).to_pandas()\n",
    "# \n",
    "# ws_frozen_sensor = (filters.unresponsive_flag(data=DataInspector.collect_data(df=df_query, feature_types=\"wind_speed\"), threshold=thr)\n",
    "#                                 & df_query.select([pl.col(feat).is_not_null().name.suffix(\"_not_null\") for feat in ws_cols])\\\n",
    "#                                   .collect(streaming=True).to_pandas()\n",
    "#                                   ).values\n",
    "# ws_frozen_sensor\n",
    "# wd_frozen_sensor = np.stack([(filters.unresponsive_flag(data=DataInspector.collect_data(df=df_query, feature_types=\"wind_direction\"), threshold=thr)\n",
    "#                                 & df_query.select(no_nulls=pl.col(f\"wind_speed_{tid}\").is_not_null())\\\n",
    "#                                   .collect(streaming=True).to_pandas()[\"no_nulls\"]\n",
    "#                                   ).values for tid in data_loader.turbine_ids], axis=1)\n",
    "# pwr_frozen_sensor = np.stack([(filters.unresponsive_flag(data=DataInspector.collect_data(df=df_query, feature_types=\"power_output\"), threshold=thr)\n",
    "#                                 & df_query.select(no_nulls=pl.col(f\"wind_speed_{tid}\").is_not_null())\\\n",
    "#                                   .collect(streaming=True).to_pandas()[\"no_nulls\"]\n",
    "#                                   ).values for tid in data_loader.turbine_ids], axis=1)\n",
    "\n",
    "# wd_frozen_sensor = filters.unresponsive_flag(data=DataInspector.collect_data(df=df_query, feature_types=\"wind_direction\"), threshold=thr).values\n",
    "# pwr_frozen_sensor = filters.unresponsive_flag(data=DataInspector.collect_data(df=df_query, feature_types=\"power_output\"), threshold=thr).values\n",
    "\n",
    "# ws_mask = lambda tid: ~ws_frozen_sensor[:, data_loader.turbine_ids.index(tid)]\n",
    "# wd_mask = lambda tid: ~wd_frozen_sensor[:, data_loader.turbine_ids.index(tid)]\n",
    "# pwr_mask = lambda tid: ~pwr_frozen_sensor[:, data_loader.turbine_ids.index(tid)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    frozen_sensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if PLOT and False:\n",
    "    plot.plot_power_curve(\n",
    "        DataInspector.collect_data(df=df_query, feature_types=\"wind_speed\"),\n",
    "        DataInspector.collect_data(df=df_query, feature_types=\"power_output\"),\n",
    "        flag=ws_frozen_sensor,\n",
    "        flag_labels=(f\"Wind Speed Unresponsive Sensors (n={ws_frozen_sensor.sum():,.0f})\", \"Normal Turbine Operations\"),\n",
    "        xlim=(-1, 15),  # optional input for refining plots\n",
    "        ylim=(-100, 3000),  # optional input for refining plots\n",
    "        legend=True,  # optional flag for adding a legend\n",
    "        scatter_kwargs=dict(alpha=0.4, s=10)  # optional input for refining plots\n",
    "    )\n",
    "\n",
    "    plot.plot_power_curve(\n",
    "        DataInspector.collect_data(df=df_query, feature_types=\"wind_speed\"),\n",
    "        DataInspector.collect_data(df=df_query, feature_types=\"power_output\"),\n",
    "        flag=wd_frozen_sensor,\n",
    "        flag_labels=(f\"Wind Direction Unresponsive Sensors (n={wd_frozen_sensor.sum():,.0f})\", \"Normal Turbine Operations\"),\n",
    "        xlim=(-1, 15),  # optional input for refining plots\n",
    "        ylim=(-100, 3000),  # optional input for refining plots\n",
    "        legend=True,  # optional flag for adding a legend\n",
    "        scatter_kwargs=dict(alpha=0.4, s=10)  # optional input for refining plots\n",
    "    )\n",
    "\n",
    "    plot.plot_power_curve(\n",
    "        DataInspector.collect_data(df=df_query, feature_types=\"wind_speed\"),\n",
    "        DataInspector.collect_data(df=df_query, feature_types=\"power_output\"),\n",
    "        flag=pwr_frozen_sensor,\n",
    "        flag_labels=(f\"Power Output Unresponsive Sensors (n={pwr_frozen_sensor.sum():,.0f})\", \"Normal Turbine Operations\"),\n",
    "        xlim=(-1, 15),  # optional input for refining plots\n",
    "        ylim=(-100, 3000),  # optional input for refining plots\n",
    "        legend=True,  # optional flag for adding a legend\n",
    "        scatter_kwargs=dict(alpha=0.4, s=10)  # optional input for refining plots\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change the values corresponding to frozen sensor measurements to null or interpolate (instead of dropping full row, since other sensors could be functioning properly)\n",
    "# fill stuck sensor measurements with Null st they are marked for interpolation later,\n",
    "if False:\n",
    "    threshold = 0.01\n",
    "    df_query = data_filter.conditional_filter(df_query, threshold, mask, features)\n",
    "# df_query = df_query.with_columns(\n",
    "#                 [pl.when(~frozen_mask[:, data_loader.turbine_ids.index(feat.split(\"_\")[-1])]).then(pl.col(feat)).alias(feat)\n",
    "#                 for features, frozen_mask in zip(\n",
    "#                     [ws_cols, wd_cols, pwr_cols], \n",
    "#                     [ws_frozen_sensor, wd_frozen_sensor, pwr_frozen_sensor])\n",
    "#                 for feat in features]\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    del frozen_sensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assess and Impute/Interpolate Turbine Missing Data from Correlated Measurements OR Split Dataset during Time Stamps for which many Turbines have Missing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_query.head().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if PLOT:\n",
    "    data_inspector.plot_time_series(df_query, turbine_ids=\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_df_continuity_columns(df, mask):\n",
    "  # change first value of continuous_shifted to false such that add_df_agg_continuity_columns catches it as a start time for a period \n",
    "  return df\\\n",
    "        .filter(mask)\\\n",
    "        .with_columns(dt=pl.col(\"time\").diff())\\\n",
    "        .with_columns(dt=pl.when(pl.int_range(0, pl.len()) == 0).then(np.timedelta64(data_loader.dt, \"s\")).otherwise(pl.col(\"dt\")))\\\n",
    "        .select(\"time\", \"dt\", cs.starts_with(\"num_missing\"), cs.starts_with(\"is_missing\"))\\\n",
    "        .with_columns(continuous=pl.col(\"dt\")==np.timedelta64(data_loader.dt, \"s\"))\\\n",
    "        .with_columns(continuous_shifted=pl.col(\"continuous\").shift(-1, fill_value=True))\n",
    "\n",
    "def add_df_agg_continuity_columns(df):\n",
    "  # if the continuous flag is True, but the value in the row before it False\n",
    "  df = df.filter(pl.col(\"continuous\") | (pl.col(\"continuous\") & ~pl.col(\"continuous_shifted\")) |  (~pl.col(\"continuous\") & pl.col(\"continuous_shifted\")))\n",
    "  start_time_cond = ((pl.col(\"continuous\") & ~pl.col(\"continuous_shifted\"))).shift() | (pl.int_range(0, pl.len()) == 0)\n",
    "  end_time_cond = (~pl.col(\"continuous\") & pl.col(\"continuous_shifted\"))\n",
    "  return pl.concat([df.filter(start_time_cond).select(start_time=pl.col(\"time\")), \n",
    "                    df.with_columns(end_time=pl.col(\"time\").shift(1)).filter(end_time_cond).select(\"end_time\")], how=\"horizontal\")\\\n",
    "            .with_columns(duration=(pl.col(\"end_time\") - pl.col(\"start_time\")))\\\n",
    "            .sort(\"start_time\")\\\n",
    "            .drop_nulls()\n",
    "\n",
    "def get_continuity_group_index(df):\n",
    "  # Create the condition for the group\n",
    "  group_number = None\n",
    "\n",
    "  # Create conditions to assign group numbers based on time ranges\n",
    "  for i, (start, end) in enumerate(df.collect().select(\"start_time\", \"end_time\").iter_rows()):\n",
    "      # print(i, start, end, duration)\n",
    "      time_cond = pl.col(\"time\").is_between(start, end)\n",
    "      if group_number is None:\n",
    "          group_number = pl.when(time_cond).then(pl.lit(i))\n",
    "      else:\n",
    "          group_number = group_number.when(time_cond).then(pl.lit(i))\n",
    "\n",
    "  # If no group is matched, assign a default value (e.g., -1) \n",
    "  # group_number = group_number.when(pl.col(\"time\") > end).then(pl.lit(i + 1))\n",
    "  group_number = group_number.otherwise(pl.lit(-1))\n",
    "  return group_number\n",
    "\n",
    "def group_df_by_continuity(df, agg_df):\n",
    "  group_number = get_continuity_group_index(agg_df)\n",
    "\n",
    "  return pl.concat([agg_df, df.with_columns(group_number.alias(\"continuity_group\"))\\\n",
    "          .filter(pl.col(\"continuity_group\") != -1)\\\n",
    "          .group_by(\"continuity_group\")\\\n",
    "          .agg(cs.starts_with(\"is_missing\").sum())\\\n",
    "          .with_columns([pl.sum_horizontal(cs.contains(col) & cs.starts_with(\"is_missing\")).alias(f\"is_missing_{col}\") for col in missing_data_cols])\\\n",
    "          .sort(\"continuity_group\")], how=\"horizontal\")\n",
    "\n",
    "def merge_adjacent_periods(agg_df):\n",
    "    # merge rows with end times corresponding to start times of the next row into the next row, until no more rows need to be merged\n",
    "    # loop through and merge as long as the shifted -1 end time + dt == the start time\n",
    "    all_times = agg_df.select(pl.col(\"start_time\"), pl.col(\"end_time\")).collect()\n",
    "    data = {\"start_time\":[], \"end_time\": []}\n",
    "    start_time_idx = 0\n",
    "    for end_time_idx in range(all_times.select(pl.len()).item()):\n",
    "        end_time = all_times.item(end_time_idx, \"end_time\") \n",
    "        if not (end_time_idx + 1 == all_times.select(pl.len()).item()) and (end_time + timedelta(seconds=data_loader.dt)  == all_times.item(end_time_idx + 1, \"start_time\")):\n",
    "            continue\n",
    "        \n",
    "        data[\"start_time\"].append(all_times.item(start_time_idx, \"start_time\"))\n",
    "        data[\"end_time\"].append(end_time)\n",
    "\n",
    "        start_time_idx = end_time_idx + 1\n",
    "\n",
    "    return pl.LazyFrame(data).with_columns((pl.col(\"end_time\") - pl.col(\"start_time\")).alias(\"duration\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if there is a short or long gap for some turbines, impute them using the imputing.impute_all_assets_by_correlation function\n",
    "#       else if there is a short or long gap for many turbines, split the dataset\n",
    "missing_col_thr = max(1, int(len(data_loader.turbine_ids) * 0.05))\n",
    "missing_duration_thr = np.timedelta64(5, \"m\")\n",
    "missing_data_cols = [\"wind_speed\", \"wind_direction\", \"nacelle_direction\"]\n",
    "\n",
    "# check for any periods of time for which more than 'missing_col_thr' features have missing data\n",
    "\n",
    "df_query2 = df_query\\\n",
    "        .with_columns([cs.contains(col).is_null().name.prefix(\"is_missing_\") for col in missing_data_cols])\\\n",
    "        .with_columns(**{f\"num_missing_{col}\": pl.sum_horizontal((cs.contains(col) & cs.starts_with(\"is_missing\"))) for col in missing_data_cols})\n",
    "\n",
    "# subset of data, indexed by time, which has <= the threshold number of missing columns\n",
    "df_query_not_missing_times = add_df_continuity_columns(df_query2, mask=pl.sum_horizontal(cs.starts_with(\"num_missing\")) <= missing_col_thr)\n",
    "\n",
    "# subset of data, indexed by time, which has > the threshold number of missing columns\n",
    "df_query_missing_times = add_df_continuity_columns(df_query2, mask=pl.sum_horizontal(cs.starts_with(\"num_missing\")) > missing_col_thr)\n",
    "\n",
    "# start times, end times, and durations of each of the continuous subsets of data in df_query_missing_times \n",
    "df_query_missing = add_df_agg_continuity_columns(df_query_missing_times)\n",
    "\n",
    "# start times, end times, and durations of each of the continuous subsets of data in df_query_not_missing_times \n",
    "# AND of each of the coninuous subsets of data in df_query_missing_times that are under the threshold duration time \n",
    "df_query_not_missing = pl.concat([add_df_agg_continuity_columns(df_query_not_missing_times), \n",
    "                                  df_query_missing.filter(pl.col(\"duration\") <= missing_duration_thr)])\\\n",
    "                          .sort(\"start_time\")\n",
    "\n",
    "df_query_missing = df_query_missing.filter(pl.col(\"duration\") > missing_duration_thr)\n",
    "\n",
    "if df_query_not_missing.select(pl.len()).collect().item() == 0:\n",
    "  raise Exception(\"Parameters 'missing_col_thr' or 'missing_duration_thr' are too stringent, can't find any eligible durations of time.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_query_not_missing.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_query_missing = merge_adjacent_periods(df_query_missing)\n",
    "df_query_not_missing = merge_adjacent_periods(df_query_not_missing)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_query_not_missing.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_query_missing = group_df_by_continuity(df_query2, df_query_missing)\n",
    "df_query_not_missing = group_df_by_continuity(df_query2, df_query_not_missing)\n",
    "df_query_not_missing = df_query_not_missing.filter(pl.col(\"duration\") >= np.timedelta64(10, 'm'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_query_not_missing.select(cs.contains(\"wind_speed_wt030\") | cs.contains(\"wind_speed_wt081\"), (pl.col(\"duration\") / np.timedelta64(data_loader.dt, \"s\"))).collect()\n",
    "\n",
    "# remove sections that contain a feature for which every value is null\n",
    "# df_query_not_missing.filter(~pl.any_horizontal((cs.starts_with(\"is_missing\") & cs.contains(\"wt\")) == ((pl.col(\"duration\") / np.timedelta64(data_loader.dt, \"s\")) + 1))).collect()\n",
    "\n",
    "\n",
    "# TODO URGENT either remove that feature altogether, or remove durations for which that feature is absent, or base imputations off full dataset and impute it from a nearby turbine\n",
    "# remove columns that contain a feature for which every value is null\n",
    "# df_query_not_missing.collect()\n",
    "# df_query_not_missing.filter(~pl.any_horizontal((cs.starts_with(\"is_missing\") & cs.contains(\"wt\")) == ((pl.col(\"duration\") / np.timedelta64(data_loader.dt, \"s\")) + 1))).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_query = df_query2.select(data_loader.available_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if PLOT:\n",
    "  # Plot number of missing wind dir/wind speed data for each wind turbine (missing duration on x axis, turbine id on y axis, color for wind direction/wind speed)\n",
    "  from matplotlib import colormaps\n",
    "  from matplotlib.ticker import MaxNLocator\n",
    "  fig, ax = plt.subplots(1, 1)\n",
    "  for feature_type, marker in zip(missing_data_cols, [\"o\", \"^\"]):\n",
    "    for turbine_id, color in zip(data_loader.turbine_ids, colormaps[\"tab20c\"](np.linspace(0, 1, len(data_loader.turbine_ids)))):\n",
    "      df = df_query_missing.select(\"duration\", f\"is_missing_{feature_type}_{turbine_id}\").collect(streaming=True).to_pandas()\n",
    "      ax.scatter(x=df[\"duration\"].dt.seconds / 3600,\n",
    "                  y=df[f\"is_missing_{feature_type}_{turbine_id}\"].astype(int),  \n",
    "      marker=marker, label=turbine_id, s=400, color=color)\n",
    "  ax.set_title(\"Occurence of Missing Wind Speed (circle) and Wind Direction (triangle) Values vs. Missing Duration, for each Turbine\")\n",
    "  ax.set_xlabel(\"Duration of Missing Values [hrs]\")\n",
    "  ax.set_ylabel(\"Number of Missing Values over this Duration\")\n",
    "  h, l = ax.get_legend_handles_labels()\n",
    "  # ax.legend(h[:len(data_loader.turbine_ids)], l[:len(data_loader.turbine_ids)], ncol=8)\n",
    "  ax.yaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "\n",
    "  # Plot missing duration on x axis, number of missing turbines on y-axis, marker for wind speed vs wind direction,\n",
    "  fig, ax = plt.subplots(1, 1)\n",
    "  for feature_type, marker in zip(missing_data_cols, [\"o\", \"^\"]):\n",
    "    df = df_query_missing.select(\"duration\", (cs.contains(feature_type) & cs.starts_with(\"is_missing\")))\\\n",
    "                            .with_columns(pl.sum_horizontal([f\"is_missing_{feature_type}_{tid}\" for tid in data_loader.turbine_ids]).alias(f\"is_missing_{feature_type}\")).collect(streaming=True).to_pandas()\n",
    "    ax.scatter(x=df[\"duration\"].dt.seconds / 3600,\n",
    "                y=df[f\"is_missing_{feature_type}\"].astype(int),  \n",
    "    marker=marker, label=feature_type, s=400)\n",
    "  ax.set_title(\"Occurence of Missing Wind Speed (circle) and Wind Direction (triangle) Values vs. Missing Duration, for all Turbines\")\n",
    "  ax.set_xlabel(\"Duration of Missing Values [hrs]\")\n",
    "  ax.set_ylabel(\"Number of Missing Values over this Duration\")\n",
    "  h, l = ax.get_legend_handles_labels()\n",
    "  # ax.legend(h[:len(missing_data_cols)], l[:len(missing_data_cols)], ncol=8)\n",
    "  ax.yaxis.set_major_locator(MaxNLocator(integer=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if more than 'missing_col_thr' columns are missing data for more than 'missing_timesteps_thr', split the dataset at the point of temporal discontinuity\n",
    "# df_query = [df.lazy() for df in df_query.with_columns(get_continuity_group_index(df_query_not_missing).alias(\"continuity_group\"))\\\n",
    "#                           .filter(pl.col(\"continuity_group\") != -1)\\\n",
    "#                           .drop(cs.contains(\"is_missing\") | cs.contains(\"num_missing\"))\n",
    "#                           .collect(streaming=True)\\\n",
    "#                           .sort(\"time\")\n",
    "#                           .partition_by(\"continuity_group\")]\n",
    "\n",
    "df_query = df_query.with_columns(get_continuity_group_index(df_query_not_missing).alias(\"continuity_group\"))\\\n",
    "                          .filter(pl.col(\"continuity_group\") != -1)\\\n",
    "                          .drop(cs.contains(\"is_missing\") | cs.contains(\"num_missing\"))\\\n",
    "                          .sort(\"time\")\n",
    "df_query.collect()\n",
    "# check that each split dataframe a) is continuous in time AND b) has <= than the threshold number of missing columns OR for less than the threshold time span\n",
    "# for df in df_query:\n",
    "#     assert df.select((pl.col(\"time\").diff(null_behavior=\"drop\") == np.timedelta64(data_loader.dt, \"s\")).all()).collect(streaming=True).item()\n",
    "#     assert (df.select((pl.sum_horizontal([(cs.numeric() & cs.contains(col)).is_null() for col in missing_data_cols]) <= missing_col_thr)).collect(streaming=True)\n",
    "#             |  ((df.select(\"time\").max().collect(streaming=True).item() - df.select(\"time\").min().collect(streaming=True).item()) < missing_duration_thr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# else, for each of those split datasets, impute the values using the imputing.impute_all_assets_by_correlation function\n",
    "# data_filter.multiprocessor = None\n",
    "# df_query2 = data_filter.fill_multi_missing_datasets(df_query, impute_missing_features=[\"wind_speed\", \"wind_direction\"], \n",
    "#                                                               interpolate_missing_features=[\"wind_speed\", \"wind_direction\", \"nacelle_direction\"], \n",
    "#                                                               available_features=data_loader.available_features)\n",
    "\n",
    "df_query2 = data_filter._fill_single_missing_dataset(df_idx=0, df=df_query, impute_missing_features=[\"wind_speed\", \"wind_direction\"], \n",
    "                                         interpolate_missing_features=[\"wind_speed\", \"wind_direction\", \"nacelle_direction\"], \n",
    "                                         available_features=data_loader.available_features, parallel=\"turbine_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_query = df_query2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_query.select(sorted(df_query.collect_schema().names())).collect().write_parquet(os.path.join(DATA_DIR, \"imputed_data.parquet\"), row_group_size=100000)\n",
    "# remerge into one dataframe for nacelle calibration and normalization (may not need to if integrated in models)\n",
    "# df_query = pl.concat([df.with_columns(continuity_group=pl.lit(i)) for i, df in enumerate(df_query)], how=\"vertical\").sort(\"time\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nacelle Calibration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find and correct wind direction offsets from median wind plant wind direction for each turbine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# turbine_ids = df_query.select(\"turbine_id\").unique().collect(streaming=True).to_numpy()[:, 0]\n",
    "\n",
    "# add the 3 degrees back to the wind direction signal\n",
    "offset = 0.0\n",
    "df_query2 = df_query.with_columns((cs.starts_with(\"wind_direction\") + offset % 360.0))\n",
    "df_query_10min = df_query2\\\n",
    "                    .with_columns(pl.col(\"time\").dt.round(f\"{10}m\").alias(\"time\"))\\\n",
    "                    .group_by(\"time\").agg(cs.numeric().drop_nulls().mean()).sort(\"time\")\n",
    "\n",
    "wd_median = np.nanmedian(df_query_10min.select(cs.starts_with(\"wind_direction\")).collect().to_numpy(), axis=1)\n",
    "wd_median = np.degrees(np.arctan2(np.sin(np.radians(wd_median)), np.cos(np.radians(wd_median))))\n",
    "wd_median = df_query_10min.select(\"time\", cs.starts_with(\"wind_direction\"), cs.starts_with(\"power_output\")).with_columns(wd_median=wd_median)\n",
    "\n",
    "yaw_median = np.nanmedian(df_query_10min.select(cs.starts_with(\"nacelle_direction\")).collect().to_numpy(), axis=1)\n",
    "yaw_median = np.degrees(np.arctan2(np.sin(np.radians(yaw_median)), np.cos(np.radians(yaw_median))))\n",
    "yaw_median = df_query_10min.select(\"time\", cs.starts_with(\"nacelle_direction\"), cs.starts_with(\"power_output\")).with_columns(yaw_median=yaw_median)\n",
    "\n",
    "def plot_wind_offset(full_df, wd_median, title):\n",
    "    _, ax = plt.subplots(1, 1)\n",
    "    for turbine_id in data_loader.turbine_ids:\n",
    "        df = full_df.filter(pl.col(f\"power_output_{turbine_id}\") >= 0).select(\"time\", f\"wind_direction_{turbine_id}\").collect()\n",
    "                            \n",
    "        ax.plot(df.select(\"time\").to_numpy().flatten(), \n",
    "                DataFilter.wrap_180(\n",
    "                            (df.select(f\"wind_direction_{turbine_id}\")\n",
    "                            - wd_median.filter(pl.col(f\"power_output_{turbine_id}\") >= 0).select(f\"wd_median\").collect()).to_numpy().flatten()),\n",
    "                            label=f\"{turbine_id}\")\n",
    "\n",
    "    # ax.legend(ncol=8)\n",
    "    ax.set_xlabel(\"Time (s)\")\n",
    "    ax.set_ylabel(\"Wind Direction - Median Wind Direction (deg)\")\n",
    "\n",
    "    ax.set_title(title)\n",
    "\n",
    "plot_wind_offset(df_query_10min, wd_median, \"Original\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_offsets = {\"turbine_id\": [], \"northing_bias\": []}\n",
    "\n",
    "# remove biases from median direction\n",
    "for turbine_id in data_loader.turbine_ids:\n",
    "    df = df_query_10min.filter(pl.col(f\"power_output_{turbine_id}\") >= 0)\\\n",
    "                  .select(\"time\", f\"wind_direction_{turbine_id}\", f\"nacelle_direction_{turbine_id}\").collect()\n",
    "                   \n",
    "    wd_bias = DataFilter.wrap_180(DataFilter.circ_mean(\n",
    "        (df.select(f\"wind_direction_{turbine_id}\")\n",
    "                                    - wd_median.filter(pl.col(f\"power_output_{turbine_id}\") >= 0).select(f\"wd_median\").collect()).to_numpy().flatten()))\n",
    "    yaw_bias = DataFilter.wrap_180(DataFilter.circ_mean(\n",
    "        (df.select(f\"nacelle_direction_{turbine_id}\")\n",
    "                                    - yaw_median.filter(pl.col(f\"power_output_{turbine_id}\") >= 0).select(f\"yaw_median\").collect()).to_numpy().flatten()))\n",
    "\n",
    "    df_offsets[\"turbine_id\"].append(turbine_id)\n",
    "    bias = -0.5 * (wd_bias + yaw_bias)\n",
    "    df_offsets[\"northing_bias\"].append(np.round(bias, 2))\n",
    "    \n",
    "    df_query_10min = df_query_10min.with_columns((pl.col(f\"wind_direction_{turbine_id}\") + bias) % 360.0, (pl.col(f\"nacelle_direction_{turbine_id}\") + bias) % 360.0)\n",
    "    df_query2 = df_query2.with_columns((pl.col(f\"wind_direction_{turbine_id}\") + bias) % 360.0, (pl.col(f\"nacelle_direction_{turbine_id}\") + bias) % 360.0)\n",
    "\n",
    "    print(f\"Turbine {turbine_id} bias from median wind direction: {df_offsets['northing_bias'][-1]} deg.\")\n",
    "\n",
    "df_offsets = pl.DataFrame(df_offsets)\n",
    "\n",
    "plot_wind_offset(df_query_10min, wd_median, \"Corrected\")\n",
    "\n",
    "# make sure we have corrected the bias between wind direction and yaw position by adding 3 deg. to the wind direction\n",
    "bias = 0\n",
    "for turbine_id in data_loader.turbine_ids:\n",
    "    df = df_query_10min.filter(pl.col(f\"power_output_{turbine_id}\") >= 0)\\\n",
    "                  .select(\"time\", f\"wind_direction_{turbine_id}\", f\"nacelle_direction_{turbine_id}\")\n",
    "                  \n",
    "    bias += DataFilter.wrap_180(DataFilter.circ_mean(df.select(pl.col(f\"wind_direction_{turbine_id}\") - pl.col(f\"nacelle_direction_{turbine_id}\")).collect().to_numpy().flatten()))\n",
    "    \n",
    "print(f\"Average Bias = {bias / len(data_loader.turbine_ids)} deg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_inspector.plot_wind_rose(df_query2, turbine_ids=\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO update df_query with df_query_10min\n",
    "df_query = df_query2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find offset to true North using wake loss profiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimization function for finding waked direction\n",
    "def gauss_corr(gauss_params, power_ratio):\n",
    "    xs = np.array(range(-int((len(power_ratio) - 1) / 2), int((len(power_ratio) + 1) / 2), 1))\n",
    "    gauss = -1 * gauss_params[2] * np.exp(-0.5 * ((xs - gauss_params[0]) / gauss_params[1])**2) + 1.\n",
    "    return -1 * np.corrcoef(gauss, power_ratio)[0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO use 6 months of 10 min resampled data\n",
    "# Find offsets between direction of alignment between pairs of turbines \n",
    "# and direction of peak wake losses. Use the average offset found this way \n",
    "# to identify the Northing correction that should be applied to all turbines \n",
    "# in the wind farm.\n",
    "from scipy.stats import norm\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "from floris import FlorisModel\n",
    "fi = FlorisModel(data_inspector.farm_input_filepath)\n",
    "\n",
    "def compute_offsets(df, turbine_pairs:list[tuple[int, int]]=None):\n",
    "    p_min = 100\n",
    "    p_max = 2500\n",
    "\n",
    "    prat_hfwdth = 30\n",
    "\n",
    "    prat_turbine_pairs = turbine_pairs or [(61,60), (51,50), (43,42), (41,40), (18,19), (34,33), (17,16), (21,22), (87,86), (62,63), (32,33), (59,60), (42,43)]\n",
    "\n",
    "    dir_offsets = []\n",
    "\n",
    "    for i in range(len(prat_turbine_pairs)):\n",
    "        i_up = prat_turbine_pairs[i][0]\n",
    "        i_down = prat_turbine_pairs[i][1]\n",
    "\n",
    "        dir_align = np.degrees(np.arctan2(fi.layout_x[i_up] - fi.layout_x[i_down], fi.layout_y[i_up] - fi.layout_y[i_down])) % 360\n",
    "\n",
    "        # df_sub = df_10min.loc[(df_10min['pow_%03d' % i_up] >= p_min) & (df_10min['pow_%03d' % i_up] <= p_max) & (df_10min['pow_%03d' % i_down] >= 0)]\n",
    "        tid_up =  f'wt{i_up + 1:03d}'\n",
    "        tid_down =  f'wt{i_down + 1:03d}'\n",
    "\n",
    "        if not (any(tid_up in feat for feat in data_loader.available_features) and any(tid_down in feat for feat in data_loader.available_features)):\n",
    "            continue\n",
    "\n",
    "        df_sub = df.filter((pl.col(f\"power_output_{tid_up}\") >= p_min) \n",
    "                                & (pl.col(f\"power_output_{tid_up}\") <= p_max) \n",
    "                                & (pl.col(f\"power_output_{tid_down}\") >= 0))\\\n",
    "                                .select(f\"power_output_{tid_up}\", f\"power_output_{tid_down}\", f\"wind_direction_{tid_up}\", f\"wind_direction_{tid_down}\")\n",
    "        \n",
    "        # df_sub.loc[df_sub['wd_%03d' % i_up] >= 359.5,'wd_%03d' % i_up] = df_sub.loc[df_sub['wd_%03d' % i_up] >= 359.5,'wd_%03d' % i_up] - 360.0\n",
    "        df_sub = df_sub.with_columns(pl.when((pl.col(f\"wind_direction_{tid_up}\") >= 359.5))\\\n",
    "                                        .then(pl.col(f\"wind_direction_{tid_up}\") - 360.0)\\\n",
    "                                        .otherwise(pl.col(f\"wind_direction_{tid_up}\")))\n",
    "\n",
    "        # df_sub[\"wd_round\"] = df_sub[f'wd_{i_up:03d}'].round()\n",
    "        df_sub = df_sub.with_columns(pl.col(f\"wind_direction_{tid_up}\").round().alias(f\"wd_round_{tid_up}\"))\n",
    "\n",
    "        df_sub = df_sub.group_by(f\"wd_round_{tid_up}\").mean().collect()\n",
    "\n",
    "        p_ratio = df_sub.select(pl.col(f\"power_output_{tid_down}\") / pl.col(f\"power_output_{tid_up}\")).to_numpy().flatten()\n",
    "\n",
    "        plt.figure()\n",
    "        _, ax = plt.subplots(1,1)\n",
    "        ax.plot(p_ratio, label=\"_nolegend_\")\n",
    "        ax.plot(dir_align * np.ones(2),[0,1.25], 'k--', label=\"Direction of Alignment\")\n",
    "        ax.grid()\n",
    "\n",
    "        nadir = np.argmin(p_ratio[np.arange(int(np.round(dir_align)) - prat_hfwdth, int(np.round(dir_align)) + prat_hfwdth + 1) % 360])\n",
    "        nadir = nadir + int(np.round(dir_align)) - prat_hfwdth\n",
    "\n",
    "        opt_gauss_params = minimize(gauss_corr, [0, 5.0, 1.0], args=(p_ratio[np.arange(nadir - prat_hfwdth, nadir + prat_hfwdth + 1) % 360]),method='SLSQP')\n",
    "\n",
    "        xs = np.array(range(-int((60 - 1) / 2),int((60 + 1) / 2),1))\n",
    "        gauss = -1 * opt_gauss_params.x[2] * np.exp(-0.5 * ((xs - opt_gauss_params.x[0]) / opt_gauss_params.x[1])**2) + 1.\n",
    "\n",
    "        ax.plot(xs + nadir, gauss,'k',label=\"_nolegend_\")\n",
    "        ax.plot(2 * [nadir + opt_gauss_params.x[0]], [0,1.25], 'r--',label=\"Direction of Measured Wake Center\")\n",
    "        ax.set_title(f\"Turbine Pair: ({i_up}, {i_down})\")\n",
    "        ax.legend()\n",
    "        ax.set_xlabel(\"Wind Direction [deg]\")\n",
    "        ax.set_ylabel(\"Power Ratio [-]\")\n",
    "        \n",
    "        dir_offset = DataFilter.wrap_180(nadir + opt_gauss_params.x[0] - dir_align)\n",
    "        print(f\"Direction offset for turbine pair ({tid_up}, {tid_down}) = {dir_offset}\")\n",
    "\n",
    "        dir_offsets = dir_offsets + [dir_offset]\n",
    "\n",
    "    if dir_offsets:\n",
    "        print(f\"Mean offset = {np.mean(dir_offsets)}\")\n",
    "        print(f\"Std. Dev. = {np.std(dir_offsets)}\")\n",
    "        print(f\"Min. = {np.min(dir_offsets)}\")\n",
    "        print(f\"Max. = {np.max(dir_offsets)}\")\n",
    "        return dir_offsets\n",
    "    else:\n",
    "        print(\"No available turbine pairs!\")\n",
    "\n",
    "\n",
    "dir_offsets = compute_offsets(df_query_10min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply Northing offset to each turbine\n",
    "for turbine_id in data_loader.turbine_ids:\n",
    "    df_query_10min = df_query_10min.with_columns((pl.col(f\"wind_direction_{turbine_id}\") - np.mean(dir_offsets)) % 360)\\\n",
    "                        .with_columns((pl.col(f\"nacelle_direction_{turbine_id}\") - np.mean(dir_offsets)) % 360)\n",
    "    \n",
    "    df_query2 = df_query2.with_columns((pl.col(f\"wind_direction_{turbine_id}\") - np.mean(dir_offsets)) % 360)\\\n",
    "                        .with_columns((pl.col(f\"nacelle_direction_{turbine_id}\") - np.mean(dir_offsets)) % 360)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine final wind direction correction for each turbine\n",
    "df_offsets = df_offsets.with_columns(northing_bias=DataFilter.wrap_180(df_offsets.select(\"northing_bias\").to_numpy().flatten() + np.mean(dir_offsets)).round(2))\n",
    "\n",
    "df_offsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verify that Northing calibration worked properly\n",
    "new_dir_offsets = compute_offsets(df_query_10min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_query = df_query2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalization & Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_query = df_query\\\n",
    "        .with_columns(((cs.starts_with(\"wind_direction\") - 180.).sin()).name.map(lambda c: \"wd_sin_\" + c.split(\"_\")[-1]),\n",
    "                      ((cs.starts_with(\"wind_direction\") - 180.).cos()).name.map(lambda c: \"wd_cos_\" + c.split(\"_\")[-1]))\\\n",
    "        .with_columns(**{f\"horizontal_ws_{tid}\": (pl.col(f\"wind_speed_{tid}\") * pl.col(f\"wd_sin_{tid}\")) for tid in data_loader.turbine_ids})\\\n",
    "        .with_columns(**{f\"vertical_ws_{tid}\": (pl.col(f\"wind_speed_{tid}\") * pl.col(f\"wd_cos_{tid}\")) for tid in data_loader.turbine_ids})\\\n",
    "        .with_columns(**{f\"nd_cos_{tid}\": ((pl.col(f\"nacelle_direction_{tid}\") - 180.).cos()) for tid in data_loader.turbine_ids})\\\n",
    "        .with_columns(**{f\"nd_sin_{tid}\": ((pl.col(f\"nacelle_direction_{tid}\") - 180.).sin()) for tid in data_loader.turbine_ids})\\\n",
    "        .drop(cs.starts_with(\"wind_speed\"), cs.starts_with(\"wind_direction\"), cs.starts_with(\"wd_sin\"), cs.starts_with(\"wd_cos\"), cs.starts_with(\"nacelle_direction\"))\\\n",
    "        .select(pl.col(\"time\"), pl.col(\"continuity_group\"), cs.contains(\"nd\"), cs.contains(\"ws\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_query.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_numeric = (cs.contains(\"ws\") | cs.contains(\"nd\"))\n",
    "df_query = DataInspector.pivot_dataframe(DataInspector.unpivot_dataframe(df_query, feature_types=[\"nd_cos\", \"nd_sin\", \"vertical_ws\", \"horizontal_ws\"])\\\n",
    "             .select(pl.col(\"time\"), pl.col(\"continuity_group\"), pl.col(\"turbine_id\"), (is_numeric - is_numeric.min()) / (is_numeric.max() - is_numeric.min()))\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_query.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_query.select(sorted(df_query.collect_schema().names())).collect().write_parquet(os.path.join(DATA_DIR, \"normalized_data.parquet\"), row_group_size=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_query.collect()\n",
    "os.path.join(DATA_DIR, \"normalized_data.parquet\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wind_forecasting_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
