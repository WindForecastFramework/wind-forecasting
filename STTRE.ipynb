{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QuNLaZnuwFxB"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "import torch\n",
    "from wind_forecasting.preprocessing.data_reader import DataReader\n",
    "\n",
    "class WindFarm(Dataset):\n",
    "    def __init__(self, data_dir, seq_len=1200):\n",
    "        self.seq_len = seq_len\n",
    "        # data = np.loadtxt(data_path, delimiter=',', skiprows=1, dtype=None)\n",
    "        data = DataReader().get_results_data([data_dir])[\"lut_LUT\"].to_pandas()\n",
    "        \n",
    "        wind_dir_cols = sorted([col for col in data.columns if \"TurbineWindDir_\" in col])\n",
    "        wind_mag_cols = sorted([col for col in data.columns if \"TurbineWindMag_\" in col])\n",
    "        turbine_yaw_cols = sorted([col for col in data.columns if \"TurbineYawAngle_\" in col])\n",
    "        turbine_power_cols = sorted([col for col in data.columns if \"TurbinePower_\" in col])\n",
    "\n",
    "        self.X = self.normalize(data.loc[:, wind_dir_cols + wind_mag_cols + turbine_yaw_cols + turbine_power_cols].to_numpy()) # 5 variables\n",
    "        self.y = data.loc[:, turbine_power_cols].to_numpy()\n",
    "        self.num_labels = len(np.unique(self.y))\n",
    "        self.len = len(self.y)\n",
    "\n",
    "    def __len__(self):\n",
    "        return (self.len - self.seq_len)-1\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = np.transpose(self.X[idx:idx+self.seq_len])\n",
    "        label = self.y[idx+self.seq_len+1]\n",
    "\n",
    "        return torch.tensor(x, dtype=torch.float), torch.tensor(label, dtype=torch.float)\n",
    "\n",
    "    def normalize(self, X):\n",
    "        X = np.transpose(X)\n",
    "        X_norm = []\n",
    "        for x in X:\n",
    "            x = (x-np.min(x)) / (np.max(x)-np.min(x))\n",
    "            X_norm.append(x)\n",
    "        return np.transpose(X_norm)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "class Uber(Dataset):\n",
    "    def __init__(self, dir, seq_len=60):\n",
    "        self.seq_len = seq_len\n",
    "        data = np.loadtxt(dir, delimiter=',', skiprows=1, dtype=None)\n",
    "        self.X = self.normalize(data[:, [0,1,2,3,4]]) # 5 variables\n",
    "        self.y = data[:, [2]]\n",
    "        self.num_labels = len(np.unique(self.y))\n",
    "        self.len = len(self.y)\n",
    "\n",
    "    def __len__(self):\n",
    "        return (self.len - self.seq_len)-1\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = np.transpose(self.X[idx:idx+self.seq_len])\n",
    "        label = self.y[idx+self.seq_len+1]\n",
    "\n",
    "        return torch.tensor(x, dtype=torch.float), torch.tensor(label, dtype=torch.float)\n",
    "\n",
    "    def normalize(self, X):\n",
    "        X = np.transpose(X)\n",
    "        X_norm = []\n",
    "        for x in X:\n",
    "            x = (x-np.min(x)) / (np.max(x)-np.min(x))\n",
    "            X_norm.append(x)\n",
    "        return np.transpose(X_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nJ26KwwvaHnr"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "class IstanbulStock(Dataset):\n",
    "    def __init__(self, dir, seq_len=40):\n",
    "        self.seq_len = seq_len\n",
    "        data = np.loadtxt(dir, delimiter=',', skiprows=1, dtype=None)\n",
    "        self.X = self.normalize(data[:, [0,1,2,3,4,5,6,7]])\n",
    "        self.y = data[:, [0]]\n",
    "        self.num_labels = len(np.unique(self.y))\n",
    "        self.len = len(self.y)\n",
    "\n",
    "    def __len__(self): # INFO: Dunder method (lookup)\n",
    "        return (self.len - self.seq_len)-1\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = np.transpose(self.X[idx:idx+self.seq_len])\n",
    "        label = self.y[idx+self.seq_len+1]\n",
    "\n",
    "        return torch.tensor(x, dtype=torch.float), torch.tensor(label, dtype=torch.float)\n",
    "\n",
    "    def normalize(self, X):\n",
    "        X = np.transpose(X)\n",
    "        X_norm = []\n",
    "        for x in X:\n",
    "            x = (x-np.min(x)) / (np.max(x)-np.min(x))\n",
    "            X_norm.append(x)\n",
    "        return np.transpose(X_norm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wdsfkmm8F4Ri"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "class AirQuality(Dataset):\n",
    "    def __init__(self, dir, seq_len=24):\n",
    "        self.seq_len = seq_len\n",
    "        data = np.loadtxt(dir, delimiter=',', skiprows=1, dtype=None)\n",
    "        self.X = self.normalize(data[:, [0,1,2,3,4,5,6,7,8,9,10,11]])\n",
    "        self.y = data[:, [4]]\n",
    "        self.num_labels = len(np.unique(self.y))\n",
    "        self.len = len(self.y)\n",
    "\n",
    "    def __len__(self):\n",
    "        return (self.len - self.seq_len)-1\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = np.transpose(self.X[idx:idx+self.seq_len])\n",
    "        label = self.y[idx+self.seq_len+1]\n",
    "\n",
    "        return torch.tensor(x, dtype=torch.float), torch.tensor(label, dtype=torch.float)\n",
    "\n",
    "    def normalize(self, X):\n",
    "        X = np.transpose(X)\n",
    "        X_norm = []\n",
    "        for x in X:\n",
    "            x = (x-np.min(x)) / (np.max(x)-np.min(x))\n",
    "            X_norm.append(x)\n",
    "        return np.transpose(X_norm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WmzqDct4x2Nd"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "class Traffic(Dataset):\n",
    "    def __init__(self, dir, seq_len=24):\n",
    "        self.seq_len = seq_len\n",
    "        data = np.loadtxt(dir, delimiter=',', skiprows=1, dtype=None)\n",
    "        self.X = self.normalize(data[:, [0,1,2,3,4,5,6,7]])\n",
    "        self.y = data[:, [7]]\n",
    "        self.num_labels = len(np.unique(self.y))\n",
    "        self.len = len(self.y)\n",
    "\n",
    "    def __len__(self):\n",
    "        return (self.len - self.seq_len)-1\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = np.transpose(self.X[idx:idx+self.seq_len])\n",
    "        label = self.y[idx+self.seq_len+1]\n",
    "\n",
    "        return torch.tensor(x, dtype=torch.float), torch.tensor(label, dtype=torch.float)\n",
    "\n",
    "    def normalize(self, X):\n",
    "        X = np.transpose(X)\n",
    "        X_norm = []\n",
    "        for x in X:\n",
    "            x = (x-np.min(x)) / (np.max(x)-np.min(x))\n",
    "            X_norm.append(x)\n",
    "        return np.transpose(X_norm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "brWb5KsPzn42"
   },
   "outputs": [],
   "source": [
    "class AppliancesEnergy1(Dataset):\n",
    "    def __init__(self, dir, seq_len=144):\n",
    "        self.seq_len = seq_len\n",
    "        data = np.loadtxt(dir, delimiter=',', skiprows=1, dtype=None)\n",
    "        self.X = self.normalize(data[:, 0:26])\n",
    "        self.y = data[:, [0]]\n",
    "        self.num_labels = len(np.unique(self.y))\n",
    "        self.len = len(self.y)\n",
    "\n",
    "    def __len__(self):\n",
    "        return (self.len - self.seq_len)-1\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = np.transpose(self.X[idx:idx+self.seq_len])\n",
    "        label = self.y[idx+self.seq_len+1]\n",
    "\n",
    "        return torch.tensor(x, dtype=torch.float), torch.tensor(label, dtype=torch.float)\n",
    "\n",
    "    def normalize(self, X):\n",
    "        X = np.transpose(X)\n",
    "        X_norm = []\n",
    "        for x in X:\n",
    "            x = (x-np.min(x)) / (np.max(x)-np.min(x))\n",
    "            X_norm.append(x)\n",
    "        return np.transpose(X_norm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dMiQJitnVlSN"
   },
   "outputs": [],
   "source": [
    "class AppliancesEnergy2(Dataset):\n",
    "    def __init__(self, dir, seq_len=144):\n",
    "        self.seq_len = seq_len\n",
    "        data = np.loadtxt(dir, delimiter=',', skiprows=1, dtype=None)\n",
    "        self.X = self.normalize(data[:, 0:26])\n",
    "        self.y = data[:, [1]]\n",
    "        self.num_labels = len(np.unique(self.y))\n",
    "        self.len = len(self.y)\n",
    "\n",
    "    def __len__(self):\n",
    "        return (self.len - self.seq_len)-1\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = np.transpose(self.X[idx:idx+self.seq_len])\n",
    "        label = self.y[idx+self.seq_len+1]\n",
    "\n",
    "        return torch.tensor(x, dtype=torch.float), torch.tensor(label, dtype=torch.float)\n",
    "\n",
    "    def normalize(self, X):\n",
    "        X = np.transpose(X)\n",
    "        X_norm = []\n",
    "        for x in X:\n",
    "            x = (x-np.min(x)) / (np.max(x)-np.min(x))\n",
    "            X_norm.append(x)\n",
    "        return np.transpose(X_norm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PFhUi5YX2PGG"
   },
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, embed_size, heads, seq_len, module, rel_emb):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        self.device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\" # or cpu\n",
    "        self.embed_size = embed_size\n",
    "        self.heads = heads\n",
    "        self.seq_len = seq_len\n",
    "        self.module = module\n",
    "        self.rel_emb = rel_emb\n",
    "        modules = ['spatial', 'temporal', 'spatiotemporal', 'output']\n",
    "        assert (modules.__contains__(module)), \"Invalid module\"\n",
    "\n",
    "\n",
    "        if module == 'spatial' or module == 'temporal':\n",
    "            self.head_dim = seq_len\n",
    "            self.values = nn.Linear(self.embed_size, self.embed_size, dtype=torch.float32)\n",
    "            self.keys = nn.Linear(self.embed_size, self.embed_size, dtype=torch.float32, device=self.device)\n",
    "            self.queries = nn.Linear(self.embed_size, self.embed_size, dtype=torch.float32, device=self.device)\n",
    "\n",
    "            if rel_emb:\n",
    "                self.E = nn.Parameter(torch.randn([self.heads, self.head_dim, self.embed_size], device=self.device))\n",
    "\n",
    "        else:\n",
    "            self.head_dim = embed_size // heads\n",
    "            assert (self.head_dim * heads == embed_size), \"Embed size not div by heads\"\n",
    "            self.values = nn.Linear(self.head_dim, self.head_dim, dtype=torch.float32)\n",
    "            self.keys = nn.Linear(self.head_dim, self.head_dim, dtype=torch.float32, device=self.device)\n",
    "            self.queries = nn.Linear(self.head_dim, self.head_dim, dtype=torch.float32, device=self.device)\n",
    "\n",
    "            if rel_emb:\n",
    "                self.E = nn.Parameter(torch.randn([1, self.seq_len, self.head_dim], device=self.device))\n",
    "\n",
    "        self.fc_out = nn.Linear(self.embed_size, self.embed_size, device=self.device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        N, _, _ = x.shape\n",
    "\n",
    "        #non-shared weights between heads for spatial and temporal modules\n",
    "        if self.module == 'spatial' or self.module == 'temporal':\n",
    "            values = self.values(x)\n",
    "            keys = self.keys(x)\n",
    "            queries = self.queries(x)\n",
    "            values = values.reshape(N, self.seq_len, self.heads, self.embed_size)\n",
    "            keys = keys.reshape(N, self.seq_len, self.heads, self.embed_size)\n",
    "            queries = queries.reshape(N, self.seq_len, self.heads, self.embed_size)\n",
    "\n",
    "        #shared weights between heads for spatio-temporal module\n",
    "        else:\n",
    "            values, keys, queries = x, x, x\n",
    "            values = values.reshape(N, self.seq_len, self.heads, self.head_dim)\n",
    "            keys = keys.reshape(N, self.seq_len, self.heads, self.head_dim)\n",
    "            queries = queries.reshape(N, self.seq_len, self.heads, self.head_dim)\n",
    "            values = self.values(values)\n",
    "            keys = self.keys(keys)\n",
    "            queries = self.queries(queries)\n",
    "\n",
    "\n",
    "        if self.rel_emb:\n",
    "            QE = torch.matmul(queries.transpose(1, 2), self.E.transpose(1,2))\n",
    "            QE = self._mask_positions(QE)\n",
    "            S = self._skew(QE).contiguous().view(N, self.heads, self.seq_len, self.seq_len)\n",
    "            qk = torch.einsum(\"nqhd,nkhd->nhqk\", [queries, keys])\n",
    "            mask = torch.triu(torch.ones(1, self.seq_len, self.seq_len, device=self.device),\n",
    "                    1)\n",
    "            if mask is not None:\n",
    "                qk = qk.masked_fill(mask == 0, float(\"-1e20\"))\n",
    "\n",
    "            attention = torch.softmax(qk / (self.embed_size ** (1/2)), dim=3) + S\n",
    "\n",
    "        else:\n",
    "            qk = torch.einsum(\"nqhd,nkhd->nhqk\", [queries, keys])\n",
    "            mask = torch.triu(torch.ones(1, self.seq_len, self.seq_len, device=self.device),\n",
    "                    1)\n",
    "            if mask is not None:\n",
    "                qk = qk.masked_fill(mask == 0, float(\"-1e20\"))\n",
    "\n",
    "            attention = torch.softmax(qk / (self.embed_size ** (1/2)), dim=3)\n",
    "\n",
    "        #attention(N x Heads x Q_Len x K_len)\n",
    "        #values(N x V_len x Heads x Head_dim)\n",
    "        #z(N x Q_len x Heads*Head_dim)\n",
    "\n",
    "        if self.module == 'spatial' or self.module == 'temporal':\n",
    "            z = torch.einsum(\"nhql,nlhd->nqhd\", [attention, values]).reshape(N, self.seq_len*self.heads, self.embed_size)\n",
    "        else:\n",
    "            z = torch.einsum(\"nhql,nlhd->nqhd\", [attention, values]).reshape(N, self.seq_len, self.heads*self.head_dim)\n",
    "\n",
    "        z = self.fc_out(z)\n",
    "\n",
    "        return z\n",
    "\n",
    "\n",
    "    def _mask_positions(self, qe):\n",
    "        L = qe.shape[-1]\n",
    "        mask = torch.triu(torch.ones(L, L, device=self.device), 1).flip(1)\n",
    "        return qe.masked_fill((mask == 1), 0)\n",
    "\n",
    "    def _skew(self, qe):\n",
    "        #pad a column of zeros on the left\n",
    "        padded_qe = F.pad(qe, [1,0])\n",
    "        s = padded_qe.shape\n",
    "        padded_qe = padded_qe.view(s[0], s[1], s[3], s[2])\n",
    "        #take out first (padded) row\n",
    "        return padded_qe[:,:,1:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h5x_4EVK2Rao"
   },
   "outputs": [],
   "source": [
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self, embed_size, heads, seq_len, module, forward_expansion, rel_emb):\n",
    "        super(EncoderBlock, self).__init__()\n",
    "        self.attention = SelfAttention(embed_size, heads, seq_len, module, rel_emb=rel_emb)\n",
    "\n",
    "        if module == 'spatial' or module == 'temporal':\n",
    "            self.norm1 = nn.BatchNorm1d(seq_len*heads)\n",
    "            self.norm2 = nn.BatchNorm1d(seq_len*heads)\n",
    "        else:\n",
    "            self.norm1 = nn.BatchNorm1d(seq_len)\n",
    "            self.norm2 = nn.BatchNorm1d(seq_len)\n",
    "\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(embed_size, forward_expansion * embed_size),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(forward_expansion*embed_size, embed_size)\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        attention = self.attention(x)\n",
    "        x = self.norm1(attention + x)\n",
    "        forward = self.feed_forward(x)\n",
    "        out = self.norm2(forward + x)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "\"\"\"\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, layers):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.layers = nn.ModuleList(layers)\n",
    "\n",
    "    def forward(self, x, attn_mask=None):\n",
    "        # x [B, L, D]\n",
    "        attns = []\n",
    "        for layer in self.layers: # equivalent to looping through each EncoderBlock above\n",
    "            x, attn = layer(x, attn_mask=attn_mask)\n",
    "            attns.append(attn)\n",
    "        return x, attns\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, embed_size, heads, seq_len, module, forward_expansion, rel_emb):\n",
    "        super(DecoderBlock, self).__init__()\n",
    "        self.attention = SelfAttention(embed_size, heads, seq_len, module, rel_emb=rel_emb)\n",
    "\n",
    "        if module == 'spatial' or module == 'temporal':\n",
    "            self.norm1 = nn.BatchNorm1d(seq_len*heads)\n",
    "            self.norm2 = nn.BatchNorm1d(seq_len*heads)\n",
    "        else:\n",
    "            self.norm1 = nn.BatchNorm1d(seq_len)\n",
    "            self.norm2 = nn.BatchNorm1d(seq_len)\n",
    "\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(embed_size, forward_expansion * embed_size),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(forward_expansion*embed_size, embed_size)\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        attention = self.attention(x)\n",
    "        x = self.norm1(attention + x)\n",
    "        forward = self.feed_forward(x)\n",
    "        out = self.norm2(forward + x)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "\"\"\"\n",
    "class Decoder(nn.Module):\n",
    "def __init__(self, layers, norm_layer=None):\n",
    "    super(Decoder, self).__init__()\n",
    "    self.layers = nn.ModuleList(layers)\n",
    "    self.norm = norm_layer\n",
    "\n",
    "def forward(self, x, cross, x_mask=None, cross_mask=None):\n",
    "    for layer in self.layers: # equivalent to looping through each DecoderBlock above\n",
    "        x = layer(x, cross, x_mask=x_mask, cross_mask=cross_mask)\n",
    "\n",
    "    if self.norm is not None:\n",
    "        x = self.norm(x)\n",
    "\n",
    "    return x\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RfZxk9vr2URd"
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, seq_len, embed_size, num_layers, heads, device,\n",
    "                 forward_expansion, module, output_size=1,\n",
    "                 rel_emb=True):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.module = module\n",
    "        self.embed_size = embed_size\n",
    "        self.device = device\n",
    "        self.rel_emb = rel_emb\n",
    "        self.fc_out = nn.Linear(embed_size, embed_size)\n",
    "\n",
    "        self.layers = nn.ModuleList(\n",
    "            [\n",
    "             EncoderBlock(embed_size, heads, seq_len, module, forward_expansion=forward_expansion, rel_emb=rel_emb)\n",
    "             for _ in range(num_layers)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            out = layer(x)\n",
    "\n",
    "        out = self.fc_out(out)\n",
    "        return out\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, seq_len, embed_size, num_layers, heads, device,\n",
    "                 forward_expansion, module, output_size=1,\n",
    "                 rel_emb=True):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.module = module\n",
    "        self.embed_size = embed_size\n",
    "        self.device = device\n",
    "        self.rel_emb = rel_emb\n",
    "        self.fc_out = nn.Linear(embed_size, embed_size)\n",
    "\n",
    "        self.layers = nn.ModuleList(\n",
    "            [\n",
    "             DecoderBlock(embed_size, heads, seq_len, module, forward_expansion=forward_expansion, rel_emb=rel_emb)\n",
    "             for _ in range(num_layers)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            out = layer(x)\n",
    "\n",
    "        out = self.fc_out(out)\n",
    "        return out\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2LWAXNXk2ZJj"
   },
   "outputs": [],
   "source": [
    "from torch.nn import functional as F\n",
    "from torch.nn import Parameter\n",
    "\n",
    "# from .lrt_linear import BBBLinear\n",
    "\n",
    "def calculate_kl(mu_q, sig_q, mu_p, sig_p):\n",
    "    kl = 0.5 * (2 * torch.log(sig_p / sig_q) - 1 +\n",
    "                (sig_q / sig_p).pow(2) + ((mu_p - mu_q) / sig_p).pow(2)).sum()\n",
    "    return kl\n",
    "\n",
    "class ModuleWrapper(nn.Module):\n",
    "    \"\"\"Wrapper for nn.Module with support for arbitrary flags and a universal forward pass\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(ModuleWrapper, self).__init__()\n",
    "\n",
    "    def set_flag(self, flag_name, value):\n",
    "        setattr(self, flag_name, value)\n",
    "        for m in self.children():\n",
    "            if hasattr(m, 'set_flag'):\n",
    "                m.set_flag(flag_name, value)\n",
    "\n",
    "    def forward(self, x):\n",
    "        for module in self.children():\n",
    "            x = module(x)\n",
    "\n",
    "        kl = 0.0\n",
    "        for module in self.modules():\n",
    "            if hasattr(module, 'kl_loss'):\n",
    "                kl = kl + module.kl_loss()\n",
    "\n",
    "        return x, kl\n",
    "\n",
    "class BBBLinear(ModuleWrapper):\n",
    "\n",
    "    def __init__(self, in_features, out_features, bias=True, priors=None):\n",
    "        super(BBBLinear, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.use_bias = bias\n",
    "        self.device = torch.device(\n",
    "            \"cuda:0\" if torch.cuda.is_available() else \"cpu\") # or cpu\n",
    "\n",
    "        if priors is None:\n",
    "            priors = {\n",
    "                'prior_mu': 0,\n",
    "                'prior_sigma': 0.1,\n",
    "                'posterior_mu_initial': (0, 0.1),\n",
    "                'posterior_rho_initial': (-3, 0.1),\n",
    "            }\n",
    "        self.prior_mu = priors['prior_mu']\n",
    "        self.prior_sigma = priors['prior_sigma']\n",
    "        self.posterior_mu_initial = priors['posterior_mu_initial']\n",
    "        self.posterior_rho_initial = priors['posterior_rho_initial']\n",
    "\n",
    "        self.W_mu = Parameter(torch.Tensor(out_features, in_features))\n",
    "        self.W_rho = Parameter(torch.Tensor(out_features, in_features))\n",
    "        if self.use_bias:\n",
    "            self.bias_mu = Parameter(torch.Tensor(out_features))\n",
    "            self.bias_rho = Parameter(torch.Tensor(out_features))\n",
    "        else:\n",
    "            self.register_parameter('bias_mu', None)\n",
    "            self.register_parameter('bias_rho', None)\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.W_mu.data.normal_(*self.posterior_mu_initial)\n",
    "        self.W_rho.data.normal_(*self.posterior_rho_initial)\n",
    "\n",
    "        if self.use_bias:\n",
    "            self.bias_mu.data.normal_(*self.posterior_mu_initial)\n",
    "            self.bias_rho.data.normal_(*self.posterior_rho_initial)\n",
    "\n",
    "    def forward(self, x, sample=True):\n",
    "\n",
    "        self.W_sigma = torch.log1p(torch.exp(self.W_rho))\n",
    "        if self.use_bias:\n",
    "            self.bias_sigma = torch.log1p(torch.exp(self.bias_rho))\n",
    "            bias_var = self.bias_sigma ** 2\n",
    "        else:\n",
    "            self.bias_sigma = bias_var = None\n",
    "\n",
    "        act_mu = F.linear(x, self.W_mu, self.bias_mu)\n",
    "        act_var = 1e-16 + F.linear(x ** 2, self.W_sigma ** 2, bias_var)\n",
    "        act_std = torch.sqrt(act_var)\n",
    "\n",
    "        if self.training or sample:\n",
    "            eps = torch.empty(act_mu.size()).normal_(0, 1).to(self.device)\n",
    "            return act_mu + act_std * eps\n",
    "        else:\n",
    "            return act_mu\n",
    "\n",
    "    def kl_loss(self):\n",
    "        kl = calculate_kl(self.prior_mu, self.prior_sigma,\n",
    "                          self.W_mu, self.W_sigma)\n",
    "        if self.use_bias:\n",
    "            kl += calculate_kl(self.prior_mu, self.prior_sigma,\n",
    "                               self.bias_mu, self.bias_sigma)\n",
    "        return kl\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, input_shape, output_size,\n",
    "                 embed_size, num_layers, forward_expansion, heads):\n",
    "\n",
    "        super(Transformer, self).__init__()\n",
    "        self.batch_size, self.num_var, self.seq_len = input_shape\n",
    "        self.device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\" # or mps\n",
    "        self.num_elements = self.seq_len*self.num_var\n",
    "        self.embed_size = embed_size\n",
    "        self.element_embedding = nn.Linear(self.seq_len, embed_size*self.seq_len)\n",
    "        self.pos_embedding = nn.Embedding(self.seq_len, embed_size)\n",
    "        self.variable_embedding = nn.Embedding(self.num_var, embed_size)\n",
    "        \n",
    "        self.temporal = Encoder(seq_len=self.seq_len,\n",
    "                                embed_size=embed_size,\n",
    "                                num_layers=num_layers,\n",
    "                                heads=self.num_var,\n",
    "                                device=self.device,\n",
    "                                forward_expansion=forward_expansion,\n",
    "                                module='temporal',\n",
    "                                rel_emb=True)\n",
    "\n",
    "        self.spatial = Encoder(seq_len=self.num_var,\n",
    "                               embed_size=embed_size,\n",
    "                               num_layers=num_layers,\n",
    "                               heads=self.seq_len,\n",
    "                               device=self.device,\n",
    "                               forward_expansion=forward_expansion,\n",
    "                               module = 'spatial',\n",
    "                               rel_emb=True)\n",
    "\n",
    "        self.spatiotemporal = Encoder(seq_len=self.seq_len*self.num_var,\n",
    "                                      embed_size=embed_size,\n",
    "                                      num_layers=num_layers,\n",
    "                                      heads=heads,\n",
    "                                      device=self.device,\n",
    "                                      forward_expansion=forward_expansion,\n",
    "                                      module = 'spatiotemporal',\n",
    "                                      rel_emb=True)\n",
    "        \n",
    "        # self.decoder = Decoder(seq_len=self.seq_len*self.num_var,\n",
    "        #                               embed_size=embed_size,\n",
    "        #                               num_layers=num_layers,\n",
    "        #                               heads=heads,\n",
    "        #                               device=self.device,\n",
    "        #                               forward_expansion=forward_expansion,\n",
    "        #                               module = 'decoder',\n",
    "        #                               rel_emb=True)\n",
    "\n",
    "        # TODO bayesian linear layer\n",
    "        # self.decoder_layer = BBBLinear(32, 16)  # Bayesian Linear Layer\n",
    "\n",
    "        # consolidate embedding dimension\n",
    "        self.fc_out1 = nn.Linear(embed_size, embed_size//2)\n",
    "        self.fc_out2 = nn.Linear(embed_size//2, 1)\n",
    "\n",
    "        # prediction\n",
    "        self.out = nn.Linear((self.num_elements*3), output_size)\n",
    "\n",
    "\n",
    "    def forward(self, x, dropout):\n",
    "        batch_size = len(x)\n",
    "\n",
    "        #process/embed input for temporal module\n",
    "        positions = torch.arange(0, self.seq_len).expand(batch_size, self.num_var, self.seq_len).reshape(batch_size, self.num_var * self.seq_len).to(self.device)\n",
    "        x_temporal = self.element_embedding(x).reshape(batch_size, self.num_elements, self.embed_size)\n",
    "        x_temporal = F.dropout(self.pos_embedding(positions) + x_temporal, dropout)\n",
    "\n",
    "        #process/embed input for spatial module\n",
    "        x_spatial = torch.transpose(x, 1, 2).reshape(batch_size, self.num_var, self.seq_len)\n",
    "        vars = torch.arange(0, self.num_var).expand(batch_size, self.seq_len, self.num_var).reshape(batch_size, self.num_var * self.seq_len).to(self.device)\n",
    "        x_spatial = self.element_embedding(x_spatial).reshape(batch_size, self.num_elements, self.embed_size)\n",
    "        x_spatial = F.dropout(self.variable_embedding(vars) + x_spatial, dropout)\n",
    "\n",
    "\n",
    "        #process/embed input for spatio-temporal module\n",
    "        positions = torch.arange(0, self.seq_len).expand(batch_size, self.num_var, self.seq_len).reshape(batch_size, self.num_var* self.seq_len).to(self.device)\n",
    "        x_spatio_temporal = self.element_embedding(x).reshape(batch_size, self.seq_len* self.num_var, self.embed_size)\n",
    "        x_spatio_temporal = F.dropout(self.pos_embedding(positions) + x_spatio_temporal, dropout)\n",
    "\n",
    "        out1 = self.temporal(x_temporal)\n",
    "        out2 = self.spatial(x_spatial)\n",
    "        out3 = self.spatiotemporal(x_spatio_temporal)\n",
    "        # enc_out = torch.cat((out1, out2, out3), 1)\n",
    "        out = torch.cat((out1, out2, out3), 1)\n",
    "\n",
    "        # TODO add decoder \n",
    "        # dec_out, a = self.decoder(dec_out, enc_out, x_mask=dec_self_mask, cross_mask=dec_enc_mask)\n",
    "\n",
    "        out = self.fc_out1(out)\n",
    "        out = F.leaky_relu(out)\n",
    "        out = self.fc_out2(out)\n",
    "        out = F.leaky_relu(out)\n",
    "        out = torch.flatten(out, 1)\n",
    "        out = self.out(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Uo9izz8mr2ev"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torchmetrics import MeanAbsolutePercentageError, MeanAbsoluteError\n",
    "\n",
    "NUM_EPOCHS = 10000\n",
    "TEST_SPLIT = 0.5\n",
    "\n",
    "def train_test(embed_size, heads, num_layers, dropout, forward_expansion, lr, batch_size, data_dir, dataset):\n",
    "\n",
    "    device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\" # or cpu\n",
    "\n",
    "    datasets = ['WindFarm', 'Uber', 'Traffic', 'AirQuality', 'AppliancesEnergy1', 'AppliancesEnergy2', 'IstanbulStock']\n",
    "    assert (datasets.__contains__(dataset)), \"Invalid dataset\"\n",
    "\n",
    "    #call dataset class\n",
    "    if dataset == 'WindFarm':\n",
    "        train_data = WindFarm(data_dir)\n",
    "    elif dataset == 'Uber':\n",
    "        train_data = Uber(data_dir)\n",
    "    elif dataset == 'Traffic':\n",
    "        train_data = Traffic(data_dir)\n",
    "    elif dataset == 'AirQuality':\n",
    "        train_data = AirQuality(data_dir)\n",
    "    elif dataset == 'AppliancesEnergy1':\n",
    "        train_data = AppliancesEnergy1(data_dir)\n",
    "    elif dataset == 'AppliancesEnergy2':\n",
    "        train_data = AppliancesEnergy2(data_dir)\n",
    "    elif dataset == 'IstanbulStock':\n",
    "        train_data = IstanbulStock(data_dir)\n",
    "\n",
    "\n",
    "\n",
    "    #split into train and test\n",
    "    dataset_size = len(train_data)\n",
    "    indices = list(range(dataset_size))\n",
    "    split = int(np.floor(TEST_SPLIT * dataset_size))\n",
    "    train_indices, test_indices = indices[split:], indices[:split]\n",
    "    train_sampler = SubsetRandomSampler(train_indices)\n",
    "    test_sampler = SubsetRandomSampler(test_indices)\n",
    "    train_dataloader = torch.utils.data.DataLoader(train_data, batch_size=batch_size,\n",
    "                                            sampler=train_sampler)\n",
    "    test_dataloader = torch.utils.data.DataLoader(train_data, batch_size=batch_size,\n",
    "                                                    sampler=test_sampler)\n",
    "\n",
    "\n",
    "\n",
    "    #define loss function, and evaluation metrics\n",
    "    mape = MeanAbsolutePercentageError().to(device)\n",
    "    mae = MeanAbsoluteError().to(device)\n",
    "    loss_fn = torch.nn.MSELoss()\n",
    "\n",
    "    inputs, _ = next(iter(train_dataloader))\n",
    "\n",
    "    model = Transformer(inputs.shape, 1, embed_size=embed_size, num_layers=num_layers,\n",
    "                        forward_expansion=forward_expansion, heads=heads).to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    val_losses = []\n",
    "    val_mae = []\n",
    "    val_mape = []\n",
    "\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        if epoch >= 5:\n",
    "            print('Epoch: ', epoch)\n",
    "            print('Average mse: ' + str(np.average(val_losses[-5:])))\n",
    "            print('Average mape: ' + str(np.average(val_mape[-5:])))\n",
    "            print('Average mae: ' + str(np.average(val_mae[-5:])))\n",
    "\n",
    "        total_loss = 0\n",
    "\n",
    "        #train loop\n",
    "        for i, data in enumerate(train_dataloader):\n",
    "            inputs, labels = data\n",
    "            optimizer.zero_grad()\n",
    "            output = model(inputs.to(device), dropout)\n",
    "            loss = loss_fn(output, labels.to(device))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss = total_loss + loss\n",
    "\n",
    "        total_loss = 0\n",
    "        train_acc = 0\n",
    "        total_mae = 0\n",
    "        total_mape = 0\n",
    "        div = 1\n",
    "\n",
    "        #test loop\n",
    "        with torch.no_grad():\n",
    "            for i, data in enumerate(test_dataloader):\n",
    "                inputs, labels = data\n",
    "                output = model(inputs.to(device), 0)\n",
    "                loss = loss_fn(output, labels.to(device))\n",
    "\n",
    "                total_mae = total_mae + mae(output, labels.to(device))\n",
    "                total_mape = total_mape + mape(output, labels.to(device))\n",
    "                total_loss = total_loss + loss\n",
    "                #div is used when the number of samples in a batch is less\n",
    "                #than the batch size\n",
    "                div = div + (len(inputs)/batch_size)\n",
    "\n",
    "        val_losses.append(total_loss.item()/div)\n",
    "        val_mae.append(total_mae.item()/div)\n",
    "        val_mape.append(total_mape.item()/div)\n",
    "\n",
    "\n",
    "\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test Wind Farm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!export PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0\n",
    "d = 32\n",
    "h = 4\n",
    "num_layers = 3\n",
    "forward_expansion = 1\n",
    "dropout = 0.1\n",
    "lr = 0.0001\n",
    "batch_size = 64\n",
    "data_dir = \"/Users/ahenry/Documents/toolboxes/wind-hybrid-open-controller/examples/floris_case_studies/lut\"\n",
    "dataset = 'WindFarm'\n",
    "\n",
    "train_test(d, h, num_layers, dropout, forward_expansion, lr, batch_size, data_dir, dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yriwM9-WURMG"
   },
   "source": [
    "Test Uber Stock\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x92oTplm2abW"
   },
   "outputs": [],
   "source": [
    "d = 32\n",
    "h = 4\n",
    "num_layers = 3\n",
    "forward_expansion = 1\n",
    "dropout = 0.1\n",
    "lr = 0.0001\n",
    "batch_size = 64\n",
    "dir = ''\n",
    "dataset = 'Uber'\n",
    "\n",
    "train_test(d, h, num_layers, dropout, forward_expansion, lr, batch_size, dir, dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jFI5auziUVdS"
   },
   "source": [
    "Test Appliances Energy 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rsdvYYqnUZke"
   },
   "outputs": [],
   "source": [
    "d = 8\n",
    "h = 4\n",
    "num_layers = 3\n",
    "forward_expansion = 1\n",
    "dropout = 0.1\n",
    "lr = 0.0001\n",
    "batch_size = 32\n",
    "dir = ''\n",
    "dataset = 'AppliancesEnergy1'\n",
    "train_test(d, h, num_layers, dropout, forward_expansion, lr, batch_size, dir, dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wD9dlY9BUhV-"
   },
   "source": [
    "Test Appliances Energy 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2swGhmNlUcZy"
   },
   "outputs": [],
   "source": [
    "d = 8\n",
    "h = 4\n",
    "num_layers = 3\n",
    "forward_expansion = 1\n",
    "dropout = 0.1\n",
    "lr = 0.0001\n",
    "batch_size = 32\n",
    "dir = ''\n",
    "dataset = 'AppliancesEnergy2'\n",
    "\n",
    "train_test(d, h, num_layers, dropout, forward_expansion, lr, batch_size, dir, dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uf14C1vDUjJE"
   },
   "source": [
    "Test BeijingPM2.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nbO8o-HkUteK"
   },
   "outputs": [],
   "source": [
    "d = 32\n",
    "h = 4\n",
    "num_layers = 3\n",
    "forward_expansion = 1\n",
    "dropout = 0.1\n",
    "lr = 0.0001\n",
    "batch_size = 256\n",
    "dir = ''\n",
    "dataset = 'AirQuality'\n",
    "\n",
    "train_test(d, h, num_layers, dropout, forward_expansion, lr, batch_size, dir, dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wYtjnNFzUt-X"
   },
   "source": [
    "Test Istanbul Stock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W9e0TqtWU0qu"
   },
   "outputs": [],
   "source": [
    "d = 32\n",
    "h = 4\n",
    "num_layers = 3\n",
    "forward_expansion = 1\n",
    "dropout = 0.1\n",
    "lr = 0.00005\n",
    "batch_size = 268\n",
    "dir = ''\n",
    "dataset = 'IstanbulStock'\n",
    "\n",
    "train_test(d, h, num_layers, dropout, forward_expansion, lr, batch_size, dir, dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8OgEMPR2U1K_"
   },
   "source": [
    "Test Metro Traffic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LMMEqxMlU5fg"
   },
   "outputs": [],
   "source": [
    "d = 32\n",
    "h = 4\n",
    "num_layers = 3\n",
    "forward_expansion = 1\n",
    "dropout = 0.1\n",
    "lr = 0.0001\n",
    "batch_size = 256\n",
    "dir = ''\n",
    "dataset = 'Traffic'\n",
    "\n",
    "train_test(d, h, num_layers, dropout, forward_expansion, lr, batch_size, dir, dataset)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuClass": "premium",
   "gpuType": "T4",
   "machine_shape": "hm",
   "provenance": []
  },
  "gpuClass": "premium",
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
