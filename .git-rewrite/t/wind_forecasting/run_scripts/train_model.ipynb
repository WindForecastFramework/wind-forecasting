{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wind_forecasting.datasets.wind_farm import KPWindFarm\n",
    "import os\n",
    "from helpers import TorchDataModule\n",
    "from wind_forecasting.models import spacetimeformer as stf\n",
    "import pytorch_lightning as pl\n",
    "import warnings\n",
    "warnings.filterwarnings(action=\"ignore\", category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"experiment\" : {\"run_name\": \"windfarm_debug\"},\n",
    "    \"data\": {\"data_path\": \"/Users/ahenry/Documents/toolboxes/wind_forecasting/examples/data/normalized_data.parquet\",\n",
    "             \"context_len\": 10, #120, # 10 minutes for 5 sec sample size,\n",
    "             \"target_len\": 10, # 120, # 10 minutes for 5 sec sample size,\n",
    "             \"target_turbine_ids\": [\"wt029\", \"wt034\", \"wt074\"],\n",
    "             \"normalize\": False, \n",
    "             \"batch_size\": 128,\n",
    "             \"workers\": 6,\n",
    "             \"overfit\": False,\n",
    "             \"test_split\": 0.15,\n",
    "             \"val_split\": 0.15,\n",
    "             \"collate_fn\": None\n",
    "             },\n",
    "    \"model\": {\"model_cls\": stf.spacetimeformer_model.Spacetimeformer_Forecaster # TODO these should all be defined in one models directory\n",
    "              },\n",
    "    \"training\": {\"grad_clip_norm\": 0.0, \"limit_val_batches\": 1.0, \"val_check_interval\": 1.0, \"debug\": False, \"accumulate\": 1.0}\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = os.getenv(\"TRAIN_LOG_DIR\")\n",
    "if log_dir is None:\n",
    "    log_dir = \"./data/TRAIN_LOG_DIR\"\n",
    "    print(\n",
    "        \"Using default wandb log dir path of ./data/TRAIN_LOG_DIR. This can be adjusted with the environment variable `TRAIN_LOG_DIR`\"\n",
    "    )\n",
    "if not os.path.exists(log_dir):\n",
    "    os.makedirs(log_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = KPWindFarm(**config[\"data\"])\n",
    "data_module = TorchDataModule(\n",
    "    dataset=dataset,\n",
    "    **config[\"data\"] \n",
    ") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecaster = config[\"model\"][\"model_cls\"](d_x=dataset.x_dim, d_yc=dataset.yc_dim, d_yt=dataset.yt_dim, \n",
    "                                          context_len=dataset.context_len, target_len=dataset.target_len, **config[\"model\"])\n",
    "forecaster.set_inv_scaler(dataset.reverse_scaling)\n",
    "forecaster.set_scaler(dataset.apply_scaling)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO there are other callbacks in train_spacetimeformer.py if we need\n",
    "import uuid\n",
    "\n",
    "filename = f\"{config['experiment']['run_name']}_\" + str(uuid.uuid1()).split(\"-\")[0]\n",
    "model_ckpt_dir = os.path.join(log_dir, filename)\n",
    "config[\"experiment\"][\"model_ckpt_dir\"] = model_ckpt_dir\n",
    "saving = pl.callbacks.ModelCheckpoint(\n",
    "    dirpath=model_ckpt_dir,\n",
    "    monitor=\"val/loss\",\n",
    "    mode=\"min\",\n",
    "    filename=f\"{config['experiment']['run_name']}\" + \"{epoch:02d}\",\n",
    "    save_top_k=1,\n",
    "    auto_insert_metric_name=True,\n",
    ")\n",
    "callbacks = [saving]\n",
    "# test_samples = next(iter(data_module.test_dataloader()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config[\"training\"][\"val_check_interval\"] <= 1.0:\n",
    "    val_control = {\"val_check_interval\": config[\"training\"][\"val_check_interval\"]}\n",
    "else:\n",
    "    val_control = {\"check_val_every_n_epoch\": int(config[\"training\"][\"val_check_interval\"])}\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    # gpus=args.gpus,\n",
    "    callbacks=callbacks,\n",
    "    logger=None,\n",
    "    accelerator=\"auto\",\n",
    "    gradient_clip_val=config[\"training\"][\"grad_clip_norm\"],\n",
    "    gradient_clip_algorithm=\"norm\",\n",
    "    overfit_batches=20 if config[\"training\"][\"debug\"] else 0,\n",
    "    accumulate_grad_batches=config[\"training\"][\"accumulate\"],\n",
    "    sync_batchnorm=True,\n",
    "    limit_val_batches=config[\"training\"][\"limit_val_batches\"],\n",
    "    **val_control,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train\n",
    "trainer.fit(forecaster, datamodule=data_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "trainer.test(datamodule=data_module, ckpt_path=\"best\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wind_forecasting_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
